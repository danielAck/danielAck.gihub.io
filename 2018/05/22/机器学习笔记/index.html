<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script>
<link href="//cdn.bootcss.com/pace/1.0.2/themes/pink/pace-theme-flash.css" rel="stylesheet">

<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  
    
      
    

    
  

  

  

  
    
      
    

    
  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|Menlo:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/blog_favicon_32×32.ico?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/blog_favicon_16×16.ico?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="机器学习," />










<meta name="description" content="对西瓜书的知识点进行一个梳理">
<meta name="keywords" content="机器学习">
<meta property="og:type" content="article">
<meta property="og:title" content="西瓜书机器学习笔记">
<meta property="og:url" content="https://danielack.github.io/2018/05/22/机器学习笔记/index.html">
<meta property="og:site_name" content="Daniel_Codezone">
<meta property="og:description" content="对西瓜书的知识点进行一个梳理">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://danielblog.oss-cn-beijing.aliyuncs.com/ML_Note/bias_var.jpg">
<meta property="og:image" content="https://danielblog.oss-cn-beijing.aliyuncs.com/ML_Note/KKT.jpg">
<meta property="og:image" content="https://danielblog.oss-cn-beijing.aliyuncs.com/ML_Note/KKT_2.png">
<meta property="og:image" content="https://danielblog.oss-cn-beijing.aliyuncs.com/ML_Note/hinge_loss_def.png">
<meta property="og:image" content="https://danielblog.oss-cn-beijing.aliyuncs.com/ML_Note/%E5%90%88%E9%A1%B5%E6%8D%9F%E5%A4%B1.png">
<meta property="og:image" content="https://danielblog.oss-cn-beijing.aliyuncs.com/ML_Note/EM.png">
<meta property="og:updated_time" content="2018-07-22T15:00:22.249Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="西瓜书机器学习笔记">
<meta name="twitter:description" content="对西瓜书的知识点进行一个梳理">
<meta name="twitter:image" content="https://danielblog.oss-cn-beijing.aliyuncs.com/ML_Note/bias_var.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://danielack.github.io/2018/05/22/机器学习笔记/"/>





  <title>西瓜书机器学习笔记 | Daniel_Codezone</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Daniel_Codezone</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">有志者，事竟成</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://danielack.github.io/2018/05/22/机器学习笔记/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Daniel_柏桦">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/head.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Daniel_Codezone">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">西瓜书机器学习笔记</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-05-22T20:27:15+08:00">
                2018-05-22
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/05/22/机器学习笔记/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count gitment-comments-count" data-xid="/2018/05/22/机器学习笔记/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/2018/05/22/机器学习笔记/" class="leancloud_visitors" data-flag-title="西瓜书机器学习笔记">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">热度&#58;</span>
               
                 <span class="leancloud-visitors-count">℃</span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <blockquote>
<p>对西瓜书的知识点进行一个梳理</p>
</blockquote>
<a id="more"></a>
<h2 id="第一章-绪论"><a href="#第一章-绪论" class="headerlink" title="第一章 绪论"></a>第一章 绪论</h2><h3 id="1-2-基本术语"><a href="#1-2-基本术语" class="headerlink" title="1.2 基本术语"></a>1.2 基本术语</h3><ul>
<li><p>所有的可能的模型组合成为一个假设空间 <strong>H</strong></p>
</li>
<li><p>预测分为两类任务：</p>
<ul>
<li><strong>分类任务</strong>： 预测的结果是离散值<ul>
<li>二分类任务，其中标签为(0/1 或者 +1/-1 代表 正例/负例)</li>
<li>多分类任务，类别数目 k &gt; 2</li>
</ul>
</li>
<li><strong>回归任务</strong>： 预测的结果是连续值</li>
</ul>
</li>
<li><p>同时另一种任务是 <strong>聚类</strong>：事先不知道数据的规律，同时没有标签，属于无监督学习</p>
</li>
<li><p>建立模型/学习 的目的</p>
<ul>
<li>找到一个从输入空间 X -&gt; 输出 Y的函数映射</li>
<li>使得学习到的模型能够很好的拟合“新数据”，即具有很好的<strong>泛化能力</strong></li>
</ul>
</li>
<li><p>通常假设样本数据服从<strong>独立同分布</strong>（i.i.d），而服从的<strong>分布是未知的</strong>，我们需要做的就是找出这个分布规律</p>
</li>
<li><p><strong>归纳偏好</strong> 可以理解为对不同特征的重视程度</p>
</li>
<li><p>任何一个有效的机器学习算法必有相应的<strong>归纳偏好</strong>，因为能够拟合有限数据的曲线并不唯一，因此需要设定归纳偏好，使得泛化能力得到保证</p>
</li>
<li><p>通常使用 <strong>奥卡姆剃刀</strong> 原则作为一般性的归纳偏好原则，但是很遗憾原则中 “简单” 的含义针对不同的模型并不唯一，需要借助其他机制才能完成判断</p>
</li>
<li><p><strong>NFL (No Free Lunch Theorem)</strong> 指出，必须结合具体的学习问题去讨论算法的优劣性</p>
</li>
<li><p>需要注意学习算法自身的偏好与问题是否匹配，不同的数据集有不同的“特点”，需要不同的“偏好”去拟合具体的数据集</p>
</li>
</ul>
<h2 id="第二章-模型评估与选择"><a href="#第二章-模型评估与选择" class="headerlink" title="第二章 模型评估与选择"></a>第二章 模型评估与选择</h2><h3 id="2-1-经验误差与过拟合"><a href="#2-1-经验误差与过拟合" class="headerlink" title="2.1 经验误差与过拟合"></a>2.1 经验误差与过拟合</h3><ul>
<li><p>误差分为两类：</p>
<ul>
<li>经验误差： 数据集上的误差</li>
<li>泛化误差： 新样本上的误差</li>
</ul>
</li>
<li><p>过拟合的理解：</p>
<ul>
<li>学习到了数据集中不够 general 的特性</li>
<li>把训练集自身的某些特性当做整体的特性</li>
<li>过拟合是无法避免的</li>
</ul>
</li>
</ul>
<h3 id="2-2-评估方法"><a href="#2-2-评估方法" class="headerlink" title="2.2 评估方法"></a>2.2 评估方法</h3><ul>
<li>模型选择中存在两个问题：<ul>
<li>泛化误差我们无从知晓</li>
<li>训练误差不靠谱(存在过拟合的情况）</li>
</ul>
</li>
</ul>
<p>因此提出不同的评估方法</p>
<ul>
<li><p>需要一个 “测试集” ，通过 “测试集” 上的测试误差作为泛化误差的估计</p>
</li>
<li><p>测试集尽量不在训练集中出现，即二者尽量互斥</p>
</li>
</ul>
<h4 id="测试集的划分方法"><a href="#测试集的划分方法" class="headerlink" title="测试集的划分方法"></a>测试集的划分方法</h4><ol>
<li><p>留出法</p>
<p> 将数据集划分为两个互斥的集合，同时保留类别比例(即进行分层抽样)</p>
<p> 一般采用确定分层比例后，在层的大小内进行若干次随机划分、重复进行后取平均值作为最终结果</p>
</li>
<li><p>交叉验证法</p>
<p> 与留出法相似，将数据集A分为 k 个互斥的集合</p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> k:</span><br><span class="line"></span><br><span class="line">	将 A - k_i 作为训练集</span><br><span class="line">	将 k_i 作为测试集</span><br><span class="line">	记录 测试误差 err_i</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> 测试误差的平均数</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>p次k折交叉验证</strong>：通常重复 p 次进行，取 p 次结果的均值</li>
<li><strong>目的</strong>: 为了减少样本划分不同而引入的误差 </li>
</ul>
</li>
<li><p>留一法</p>
<p> k 折交叉验证法的特殊情况<br> 当 k = m , 此时称为 <strong>留一法</strong></p>
</li>
<li><p>自助法</p>
<p> 在 <strong>数据集较小、很难划分 训练集/ 测试集</strong> 的时候很有效</p>
<p> 但是，此方法产生的数据改变了出事数据集的分布，会引入估计偏差</p>
<p> 具体方法：<br> 假设数据集 D 有 m 个样本</p>
<ol>
<li>每次从数据集 D 中采出一个样本，放入 D‘ 中，再将样本放回 D</li>
<li>重复进行 m 次采样操作</li>
<li><p>用 D’ 作为训练集， D / D{`} 作为测试集</p>
<p>可证明，测试集中度数据占 D 中的 1/3 ， 且均未出现过在 D‘ 中</p>
<p>自助法在集成学习中的 Bagging 模型中还会用到（抽样方法）</p>
</li>
</ol>
</li>
</ol>
<p><strong>验证集</strong></p>
<ul>
<li><p>我们将数据集划分为训练集和验证集，在验证集上进行模型选择和调参</p>
<blockquote>
<p><strong>注意！！！</strong><br>在模型选择调节完毕之后，还应将整个数据集 D 再完整训练一遍！</p>
</blockquote>
</li>
</ul>
<h3 id="2-3-性能度量"><a href="#2-3-性能度量" class="headerlink" title="2.3 性能度量"></a>2.3 性能度量</h3><ul>
<li>不同的性能度量导致不同的评判结果</li>
<li>不能仅仅依赖评判结果，不同的任务需求和数据需要不同的模型和性能度量</li>
</ul>
<h4 id="2-3-1-错误率与精度"><a href="#2-3-1-错误率与精度" class="headerlink" title="2.3.1 错误率与精度"></a>2.3.1 错误率与精度</h4><blockquote>
<p>分类任务中最常用的性能度量，可用于 <strong>二分类</strong> 及 <strong>多分类</strong></p>
</blockquote>
<h4 id="2-3-2-查准率、查全率与-F1"><a href="#2-3-2-查准率、查全率与-F1" class="headerlink" title="2.3.2 查准率、查全率与 F1"></a>2.3.2 查准率、查全率与 F1</h4><blockquote>
<p>查准率 也叫 准确率（Precision）<br>查全率 也叫 召回率（Recall）</p>
</blockquote>
<p><strong>Precision</strong> 和 <strong>Recall</strong> 定义为：</p>
<script type="math/tex; mode=display">{P} = TP \over {TP + FP}</script><script type="math/tex; mode=display">{R} = TP \over {TP + FN}</script><ul>
<li>Precision 很高往往导致 Recall 较低，可以理解为谨小慎微，需要很大的确信度才会做决定</li>
<li>Recall 很高往往导致 Precision 较低，可以理解为宁可错杀一万，也不会放过一个</li>
</ul>
<p><strong>P-R 曲线</strong></p>
<ol>
<li><p>绘制具体步骤</p>
<p> 参考ROC曲线的绘制过程，将预测结果排序，依次将预测结果设置为阈值然后计算相应的 Precision 和 Recall    </p>
</li>
<li><p>若分类器的 A 的 P-R 曲线完全被另一个分类器B “包住” ,则 B 的性能优于 A</p>
</li>
<li><p>平衡点（BEP）</p>
<p> 是当 Precision = Recall 时的取值，BEP 大的分类器更优</p>
</li>
</ol>
<p><strong>F1 Score</strong></p>
<blockquote>
<p>比算术平均、几何平均更加重视较小值</p>
</blockquote>
<script type="math/tex; mode=display">{F_1} = \frac {2 \times P \times R}{P + R}</script><p><strong>Fβ Score</strong></p>
<blockquote>
<p>针对对于查准率和查全率重视程度不一样的情况<br>比算术平均、几何平均更加重视较小值</p>
</blockquote>
<script type="math/tex; mode=display">{F_\beta} =  \frac {(1 + {\beta}^2) \times P \times R}{({\beta}^2 \times P) + R}</script><p><strong>macro F1 Score</strong></p>
<blockquote>
<p>亦称 宏F1<br>直接计算，再求平均</p>
</blockquote>
<script type="math/tex; mode=display">{macro-P} = {1 \over n} \sum_{i=1}^{n}P_i</script><p>同理可得 macro-R，利用 macro-P、macro-R 和 F1 计算公式最后可得macro-F1</p>
<p><strong>micro F1 Score</strong><br>亦称 微F1，同理macro-F1，记为micro-F1，先对 <strong>TP、TN、FP、FN</strong> 求平均，再计利用平均值计算F1 Score</p>
<h4 id="2-3-3-ROC与AUC"><a href="#2-3-3-ROC与AUC" class="headerlink" title="2.3.3 ROC与AUC"></a>2.3.3 ROC与AUC</h4><ul>
<li><strong>ROC</strong> - 受试者工作特征</li>
<li>横轴： TPR ； 纵轴： FPR</li>
</ul>
<hr>
<ul>
<li><strong>AUC</strong> - 曲线下面积</li>
<li>面积的大小代表预测的准确性</li>
<li>绘制步骤<ul>
<li>对每个测试样例进行预测，得到预测结果</li>
<li>对预测结果进行排序</li>
<li>将分类阈值一次设置为每一个预测值即将每个样例设置为正例）</li>
<li>设置好阈值之后，循环其他样例，若样例为真正例，则纵坐标加<script type="math/tex">1 \over m^+</script></li>
<li>AUC中坐标 <script type="math/tex">x</script> 代表 <strong>该点之前的反例所占比</strong>，因为每遇到一个反例横坐标扩大一个百分点（<script type="math/tex">1 \over m^-</script>），同理，<script type="math/tex">y</script> 代表 <strong>该点之前的正例所占比</strong></li>
<li>AUC的范围是 <strong>[0.5, 1]</strong></li>
</ul>
</li>
</ul>
<h4 id="2-3-4-代价敏感错误率与代价曲线"><a href="#2-3-4-代价敏感错误率与代价曲线" class="headerlink" title="2.3.4 代价敏感错误率与代价曲线"></a>2.3.4 代价敏感错误率与代价曲线</h4><p>敏感代价错误率</p>
<script type="math/tex; mode=display">E(f;D;cost) = \frac{1}{m} (\sum_{x \in {D^+}}I(f(x_i)\not=y_i) \times cost_{01} + \sum_{x_i \in D^-}I(f(x_i)\not=y_i) \times cost_{10})</script><ul>
<li>为了权衡不同类型的错误造成的代价</li>
<li>使用 <strong>代价矩阵</strong>(<script type="math/tex">cost_{01}，cost_{10}...</script>)刻画不同的代价</li>
<li>使用 <strong>比值</strong> 刻画代价之间的重要程度</li>
</ul>
<h3 id="2-4-比较检验"><a href="#2-4-比较检验" class="headerlink" title="2.4 比较检验"></a>2.4 比较检验</h3><ul>
<li>使用假设检验的方法，进行误差的假设检验</li>
<li>后期进行二项检验、t检验的具体笔记</li>
</ul>
<h3 id="2-5-偏差与方差"><a href="#2-5-偏差与方差" class="headerlink" title="2.5 偏差与方差"></a>2.5 偏差与方差</h3><ul>
<li>针对回归问题，期望泛化误差可以分解为 <strong>偏差、方差、噪声之和</strong></li>
<li>分解结果：</li>
</ul>
<script type="math/tex; mode=display">E(f;D) = bias^2(x) + var(x) + \epsilon^2</script><ul>
<li>具体分解过程见书 P45页</li>
<li><strong>偏差与方差的关系</strong><ul>
<li>偏差与方差是 <strong>准</strong> 与 <strong>确</strong> 的关系</li>
<li>偏差 ：刻画预测结果的 <strong>确</strong> 的程度，即算法本身的拟合能力</li>
<li>方差 ：刻画预测结果的 <strong>准</strong> 的程度，也提现算法抗数据扰动的能力</li>
<li>噪声 ： 刻画算法达到期望泛化误差的下界，即学习问题本身的难度</li>
<li>学习初期， <strong>偏差</strong> 主导泛化错误率，体现为 <strong>欠拟合</strong></li>
<li>学习后期，欠拟合问题逐渐解决， <strong>方差</strong> 主导泛化错误率，体现为 <strong>过拟合</strong> ，在学习器学习过程中，会渐渐学习到数据中的 <strong>干扰数据</strong>，从而导致学习器的抗干扰能力下降，也即方差增加</li>
</ul>
</li>
</ul>
<p>偏差与方差的经典关系图：<br><img src="https://danielblog.oss-cn-beijing.aliyuncs.com/ML_Note/bias_var.jpg" alt=""></p>
<ul>
<li>Min-Max规范化和z-score规范化的区别</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th><strong>Min-Max</strong></th>
<th><strong>z-score</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td>计算量小</td>
<td>计算量大</td>
</tr>
<tr>
<td>易受极大/极小值影响</td>
<td>不易受极值影响</td>
</tr>
<tr>
<td>超出原极值范围才需要重新计算</td>
<td>来一个数据重新计算一次</td>
</tr>
</tbody>
</table>
</div>
<h2 id="第3章-线性模型"><a href="#第3章-线性模型" class="headerlink" title="第3章 线性模型"></a>第3章 线性模型</h2><h3 id="3-1-基本形式"><a href="#3-1-基本形式" class="headerlink" title="3.1 基本形式"></a>3.1 基本形式</h3><ul>
<li>试图用 <strong>属性</strong> 的 <strong>线性组合</strong> 进行预测</li>
<li>基本形式：</li>
</ul>
<script type="math/tex; mode=display">f(x) = \omega^Tx + b = \omega_1x_1 + \omega_2x_2 + \cdots + \omega_dx_d + b</script><ul>
<li>$\omega$ 代表了各属性的 <strong>重要性</strong>， 因此线性模型具有很好的 <strong>可解释性</strong></li>
</ul>
<h3 id="3-2-线性回归"><a href="#3-2-线性回归" class="headerlink" title="3.2 线性回归"></a>3.2 线性回归</h3><p>回归模型的变量通常是连续值，因此当属性是离散值时，可以进行 <strong>连续化</strong></p>
<ul>
<li>当属性件存在 “序” 的关系，通过对离散值进行排序，然后转换为相应大小的连续值</li>
<li>当属性间不存在 “序” 的关系，可以通过使用 <strong>OneHot</strong> 编码方法进行离散值的转换</li>
<li>无序属性进行连续化会不恰当的引入 “序” 的关系</li>
</ul>
<p>损失函数：</p>
<ul>
<li>使用 <strong>均方误差(MSE)</strong> 进行性能度量</li>
</ul>
<script type="math/tex; mode=display">MSE = \sum_{i=1}^{m}{(f(x_i) - y_i)}^2 = \sum_{i=1}^{m}{(\omega x_i + b - y_i)}^2</script><p>求解方法：</p>
<ul>
<li>最小二乘法 - 找到一条直线使所有的样本到直线的欧氏距离最小</li>
<li>最大似然估计 - 求出使所有出现的结果值的概率最大的概率模型参数</li>
</ul>
<h3 id="3-3-对数几率回归"><a href="#3-3-对数几率回归" class="headerlink" title="3.3 对数几率回归"></a>3.3 对数几率回归</h3><p>也叫逻辑回归（Logistic Regression），虽然叫回归，但是其实是 <strong>分类</strong> 算法</p>
<p>起源</p>
<ul>
<li>希望使用 <strong>线性模型</strong> 进行 <strong>分类预测任务</strong></li>
<li>因此输出结果需要是 <strong>离散的</strong> 类别标签，二分类为例： <strong>y = {0,1}</strong></li>
<li>但是线性模型的预测结果是连续值，因此我们需要找到一个函数将连续的结果值映射为离散的类别标签</li>
<li>因此，<strong>单位阶跃函数</strong> 是理想的映射函数，但是单位阶跃函数并不单调可微，这样就引入了 <strong>对数几率函数</strong> 作为替代函数</li>
<li>预测的结果其实是 <strong>对率几率</strong>，定义如下：</li>
</ul>
<script type="math/tex; mode=display">\omega x^T + b = ln \frac {y}{1-y}</script><ul>
<li>而 <strong>y</strong> 才是概率</li>
<li>去除 ln 有 <strong>几率</strong> 的定义，反映 <strong>x</strong> 作为正例的相对可能性</li>
<li>直接对 <strong>分类可能性</strong> 建模，无需实现假设数据分布，避免假设分布不准确引入的问题</li>
</ul>
<h3 id="3-4-线性判别分析"><a href="#3-4-线性判别分析" class="headerlink" title="3.4 线性判别分析"></a>3.4 线性判别分析</h3><p><strong>核心思想：</strong></p>
<ul>
<li>异类点的投影点尽可能的远离，同类点之间的协方差尽可能的小(即同类点之间尽可能的靠近)</li>
</ul>
<p><strong>优化目标</strong></p>
<script type="math/tex; mode=display">J = \frac{w^TS_bw}{w^TS_ww}</script><ul>
<li>二维情况详细推导可以参考 <a href="https://www.cnblogs.com/engineerLF/p/5393119.html" target="_blank" rel="noopener">https://www.cnblogs.com/engineerLF/p/5393119.html</a></li>
<li>其中可以用“广义瑞利商”的性质得出最优<script type="math/tex">w</script> ,详细性质推导见 <a href="https://www.cnblogs.com/pinard/p/6244265.html" target="_blank" rel="noopener">https://www.cnblogs.com/pinard/p/6244265.html</a></li>
</ul>
<p><strong>最大化方法</strong></p>
<ul>
<li>添加负号，变为最小化问题</li>
<li>使用拉格朗日乘子法，</li>
</ul>
<p><strong>求解结果</strong></p>
<ul>
<li>在一般的实践中，<script type="math/tex">S_w^{-1}</script> 使用SVD进行奇异值分解，通过 <script type="math/tex">S^{-1}_w = V{\Sigma}^{-1}U^T</script> 得到 <script type="math/tex">{S_w}^{-1}</script></li>
</ul>
<p><strong>KLDA</strong></p>
<ul>
<li>在LDA中引入核函数，应对非线性可分问题，具体推导见西瓜书P137</li>
</ul>
<h3 id="3-5-多分类学习"><a href="#3-5-多分类学习" class="headerlink" title="3.5 多分类学习"></a>3.5 多分类学习</h3><ul>
<li><p>三种拆分策略：</p>
<ul>
<li>一对一（OvO）</li>
<li>一对其余（OvR）</li>
<li><strong>多对多</strong>（MvM）</li>
</ul>
</li>
<li><p><strong>一对一</strong> 将类别两两配对生成 $\frac {N(N-2)}{2}$ 个分类器</p>
</li>
<li><strong>一对其余</strong> 每次将一个类别视为正例，其余视为反例，因此一共会生成 $N$ 个分类器</li>
<li>但是，一对一的 训练开销 通常小于 一对其余，因为一对一只用到了两个类的训练数据</li>
</ul>
<h4 id="多对多分类"><a href="#多对多分类" class="headerlink" title="多对多分类"></a>多对多分类</h4><ul>
<li>常用纠错输出码（ECOC）</li>
<li>常见编码矩阵：二元码，三元码（加上停用类，不带入计算）</li>
<li>将类别进行随机拆分，拆分为两类：正例和反例，形成很多个二分类器</li>
<li>任意两个分类器越 <strong>”不相同“</strong>，训练结果越好，思想类似于 Bagging，训练多个高多样性的分类器</li>
<li>最后这些分类器对同一个样本预测，将预测结果分别计算编码距离（海明距离 / 欧式距离），将编码距离最小的类作为最终结果</li>
<li><strong>同一个分类任务</strong>， ECOC编码越长，分类器之间的相似性就会越小，因此容错能力越好</li>
<li><strong>同等长度的编码</strong>， 任意两个类别的编码距离越大，容错能力越好</li>
</ul>
<h3 id="3-6-类别不平衡问题"><a href="#3-6-类别不平衡问题" class="headerlink" title="3.6 类别不平衡问题"></a>3.6 类别不平衡问题</h3><h4 id="“再缩放”-思想"><a href="#“再缩放”-思想" class="headerlink" title="“再缩放” 思想"></a>“再缩放” 思想</h4><p>在对分类任务进行预测时，<strong>y</strong> 是代表正例的概率，通常将阈值设置为 0.5，即 $y &gt; 0.5$ ,将结果判定为正例，否则判定为负例</p>
<p>言外之意：</p>
<script type="math/tex; mode=display">若 y > 0.5, 预测为正例</script><script type="math/tex; mode=display">即若 \frac{y}{1-y} > 1, 预测为正例</script><p>但是，注意这样做的原因：<br><strong>0.5</strong> 代表的其实是 <strong>观测几率</strong>, 即我们在数据集中能观测到正例的几率（我们 <strong>假设</strong> 两个类别的数据集是一样大的，因此观测几率是 <strong>0.5</strong>）所以当 $y &gt; 0.5$ 代表预测正例的概率大于观测概率，当然应该判定为正例。</p>
<p>而现在，不同的类别对应的数据量不同了，不再是 1 ： 1，我们的阈值就不应该设置为 <strong>0.5</strong>， 而是应该当：</p>
<script type="math/tex; mode=display">若 \frac {y}{1-y} > \frac{m^+}{m^-} ，预测为正例</script><p>解出 <strong>y</strong> ，就是我们新的阈值</p>
<h4 id="欠采样、过采样思想"><a href="#欠采样、过采样思想" class="headerlink" title="欠采样、过采样思想"></a>欠采样、过采样思想</h4><p><strong>欠采样</strong></p>
<ul>
<li>减少数据集的数目，使得所有类别的数据相同</li>
<li>代表算法： EasyEnsemble，即分割负例为不同的数据集，以供多个学习器使用，利用了集成的思想</li>
</ul>
<p><strong>过采样</strong></p>
<ul>
<li>增加数据集的数目，使得所有类别的数据相同</li>
<li>需要注意增加数据集的方法，单纯的 <strong>重复采样</strong> 会导致严重的 <strong>过拟合</strong></li>
<li>代表算法： SMOTE</li>
</ul>
<h3 id="3-7-相关思考"><a href="#3-7-相关思考" class="headerlink" title="3.7 相关思考"></a>3.7 相关思考</h3><ul>
<li><p><strong>线性回归推导中的最小二乘和极大似然的关系</strong></p>
<ul>
<li><p>是因为极大似然中假设 <strong>误差服从正态分布</strong> 所以才会有推导结果与最小二乘是一样的，正态分布中刚好有二次项</p>
</li>
<li><p>另一种说法是最小二乘选择使用平方的原因是推导极大似然的推导结果就是最小二乘，所以说最小二乘是极大似然估计的结果</p>
</li>
</ul>
</li>
</ul>
<h2 id="第4章-决策树"><a href="#第4章-决策树" class="headerlink" title="第4章 决策树"></a>第4章 决策树</h2><h3 id="4-1-决策树模型与学习"><a href="#4-1-决策树模型与学习" class="headerlink" title="4.1 决策树模型与学习"></a>4.1 决策树模型与学习</h3><p><strong>优点</strong></p>
<ul>
<li>模型具有可读性，分类速度快</li>
</ul>
<p><strong>概念</strong></p>
<ul>
<li>损失函数使用 <strong>正则化的极大似然函数</strong></li>
<li>学习的策略是 <strong>最小化损失函数</strong></li>
<li>使用启发式方法求解决策树，求解的结果是<strong>次优的</strong></li>
<li>结点有两种类型：内部结点(非叶子结点）表示一个特征或属性， 叶子结点代表一个类</li>
<li>还表示给定特征条件下的概率分布</li>
</ul>
<p>决策树的学习算法包括：</p>
<ul>
<li><strong>特征选择</strong>（划分选择）</li>
<li><strong>决策树的生成</strong>，只考虑局部最优</li>
<li><strong>决策树的剪枝</strong>，考虑全局最优</li>
</ul>
<p>注意两种情况：</p>
<ol>
<li>当属性集为空或所有样本在属性集上的取值相同，将类别标记为 <strong>D</strong> 中最多的类别，使用的是当前结点的 <strong>后验分布</strong></li>
<li>当当前结点包含的样本集为空，同样将类别标记为 <strong>D</strong> 中最多的类别，但是是利用的当前结点的 <strong>先验分布</strong></li>
</ol>
<h3 id="4-2-特征选择（划分选择）"><a href="#4-2-特征选择（划分选择）" class="headerlink" title="4.2 特征选择（划分选择）"></a>4.2 特征选择（划分选择）</h3><p>常用 3 种指标选择应作为划分的特征</p>
<ul>
<li>信息增益<ul>
<li>对 <strong>可取数目较多</strong> 的特征(属性)有所偏好</li>
</ul>
</li>
<li>增益率<ul>
<li>对 <strong>可取数目较少</strong> 的特征(属性)有所偏好</li>
</ul>
</li>
</ul>
<p>所以通常采用一种折中的方法： 先找出高于平均水平的 <strong>信息增益</strong>， 再找出其中 <strong>信息增益率</strong> 最大的特征(属性)</p>
<ul>
<li>基尼系数</li>
</ul>
<h3 id="4-3-决策树的生成"><a href="#4-3-决策树的生成" class="headerlink" title="4.3 决策树的生成"></a>4.3 决策树的生成</h3><p>常用的 3 种生成算法：</p>
<ul>
<li>ID3， 使用 <strong>信息增益</strong> 进行特征选择</li>
<li>C4.5， 使用 <strong>增益率</strong> 进行特征选择</li>
<li>CART， 使用 <strong>基尼系数</strong> 进行特征选择</li>
</ul>
<h3 id="4-4-决策树的剪枝"><a href="#4-4-决策树的剪枝" class="headerlink" title="4.4 决策树的剪枝"></a>4.4 决策树的剪枝</h3><p>剪枝策略</p>
<ul>
<li>预剪枝</li>
<li>后剪枝</li>
</ul>
<p><strong>预剪枝</strong></p>
<ul>
<li>在划分结点前估计，若划分结点后不能提升泛化性能，则停止划分</li>
<li>使用贪心的策略，有可能陷入局部最优</li>
<li>带来欠拟合的风险</li>
</ul>
<p><strong>后剪枝</strong></p>
<ul>
<li>等决策树完全生成好之后，再从下而上的进行剪枝</li>
<li>剪枝策略：<ul>
<li>将结点的子树替换为叶节点，观察泛化性能是否提升，如果提升则将子树替换为叶节点，否则不进行剪枝</li>
<li>根据奥卡姆剃刀原则，如果泛化性能不变依然应该选择剪枝，形成更简单的决策树</li>
</ul>
</li>
<li>可以较好的防止过拟合，但是时间开销很大</li>
</ul>
<p>这里的泛化性能评估标准可以是 <strong>精度</strong></p>
<h3 id="4-5-连续与缺失值"><a href="#4-5-连续与缺失值" class="headerlink" title="4.5 连续与缺失值"></a>4.5 连续与缺失值</h3><h4 id="4-5-1-连续值的处理"><a href="#4-5-1-连续值的处理" class="headerlink" title="4.5.1 连续值的处理"></a>4.5.1 连续值的处理</h4><p>假设属性 <strong>A</strong> 为连续值属性， <strong>A</strong> 属性共有 <strong>N</strong> 个不同的值</p>
<ul>
<li>将这 <strong>N</strong> 个不同的值进行排序</li>
<li>依次选取划分点 <strong>t</strong>，大于划分点 <strong>t</strong> 的记为 $D_t^+$, 同理，小于 <strong>t</strong> 的记为 $D_t^-$</li>
<li>这样，可以将每一个取值都作为一次划分点，然后计算 <strong>信息增益</strong>, 最后可以得到属性 <strong>A</strong> 的信息增益</li>
<li>后续就跟离散值的做法一样了</li>
</ul>
<h4 id="4-5-2-缺失值的处理"><a href="#4-5-2-缺失值的处理" class="headerlink" title="4.5.2 缺失值的处理"></a>4.5.2 缺失值的处理</h4><p>当计算属性 A 的信息增益时，如果发现有样本有缺失值：</p>
<ul>
<li>初始将所有样本赋予 <strong>1</strong> 的 <strong>权值</strong></li>
<li>计算 <strong>属性a</strong> 上的三个概率（主要都是针对不含有缺失值的样本）<ul>
<li>无缺失样本所占总体数据比例</li>
<li>无缺失样本中第<script type="math/tex">k</script>类所占比例</li>
<li>属性 a 上无缺失样本中取值为<script type="math/tex">a^v</script>所占比例</li>
</ul>
</li>
<li>利用这三个概率计算每一个属性 <strong>a</strong> 的信息增益</li>
</ul>
<script type="math/tex; mode=display">Gain(D,a) = \rho \times (Ent(D^n) - \sum_{v=1}^{V}{r_v}^~Ent({D_v}^n))</script><ul>
<li><strong>更新含有缺失值的样本的权值，并划入每一个结点</strong><ul>
<li>将划入的数据权值更新为 <script type="math/tex">{r^n}_v * w_x</script>, 相当于将数据依概率进行划分</li>
</ul>
</li>
<li>后续与普通的操作相同</li>
</ul>
<p><strong>注意：</strong></p>
<ul>
<li>更新权值相当于以不同的概率将样本划分进每一个结点</li>
<li>因为含有缺失值的样本可能接下来还要参加其他属性的信息增益的计算，而计算中会涉及权值，所以 <strong>权值</strong> 相当于对带有缺失值的样本的 <strong>影响力</strong></li>
</ul>
<h3 id="4-6-多变量决策树"><a href="#4-6-多变量决策树" class="headerlink" title="4.6 多变量决策树"></a>4.6 多变量决策树</h3><p>相当于对特征进行了一个线性的组合，之前的特征是单变量，组合之后单个结点相当于一个 <strong>线性的分类器</strong></p>
<h3 id="4-7-习题思考"><a href="#4-7-习题思考" class="headerlink" title="4.7 习题思考"></a>4.7 习题思考</h3><p><strong>1. 以“最小训练误差”作为划分选择的缺点</strong></p>
<ul>
<li>由于训练集总会与模型之间存在误差，如果选择最小误差，则会很大程度的拟合为符合训练集的决策树，从而导致严重的过拟合</li>
</ul>
<p><strong>2. 对比使用递归、栈（非递归）、队列（非递归）三种结构生成决策树的优缺点</strong></p>
<ul>
<li><p>缺点：</p>
<ul>
<li>使用递归的方法会保存大量的参数信息，而在参数中有大量的数据信息，从而会导致内存溢出</li>
<li>如果使用栈的结构，不能完成以MaxNode的约束，会生成畸形的决策树（只有一边），只能完成以MaxDepth的约束</li>
<li>如果使用队列的结构，只能完成BFS即以MaxNode的约束</li>
</ul>
</li>
<li><p>优点：</p>
<ul>
<li>使用递归的方法代码更加简洁，易读</li>
<li>使用栈的结构有利于后剪枝</li>
<li>使用队列的结构，由于入队之前必须进行划分，如果是用预剪枝则会很方便，因为可以比较划分与否的验证集精度，而栈则做不到</li>
</ul>
</li>
</ul>
<p>当数据属性相对较多，属性不同取值相对较少时，树会比较宽，此时深度优先所需内存较小，反之宽度优先较小。</p>
<h2 id="第5章-神经网络"><a href="#第5章-神经网络" class="headerlink" title="第5章 神经网络"></a>第5章 神经网络</h2><p>后续再写</p>
<h2 id="第6章-支持向量机"><a href="#第6章-支持向量机" class="headerlink" title="第6章 支持向量机"></a>第6章 支持向量机</h2><p>支持向量机内容顺序主要根据《统计学习方法》中的章节来安排，主要内容也由《统计学习方法》展开，其中穿插入西瓜书的知识点</p>
<h3 id="6-1-概念"><a href="#6-1-概念" class="headerlink" title="6.1 概念"></a>6.1 概念</h3><ul>
<li>SVM是一种 <strong>二分类</strong> 模型</li>
<li>学习算法是 <strong>求解凸二次规划的最优化算法</strong></li>
<li>模型分类三类<ul>
<li>线性可分支持向量机：当数据集线性可分（硬间隔支持向量机）</li>
<li>线性支持向量机：当数据集近似线性可分（软间隔支持向量机）</li>
<li>非线性支持向量机：使用核技巧</li>
</ul>
</li>
<li>核函数<ul>
<li>表示将输入从输入空间映射到特征空间得到的<strong>特征向量之间的内积</strong></li>
</ul>
</li>
</ul>
<h3 id="6-2-线性可分支持向量机"><a href="#6-2-线性可分支持向量机" class="headerlink" title="6.2 线性可分支持向量机"></a>6.2 线性可分支持向量机</h3><ul>
<li>输入空间<ul>
<li>输入的数据向量所在的空间</li>
</ul>
</li>
<li>特征空间<ul>
<li>特征向量所在的空间</li>
</ul>
</li>
</ul>
<p>支持向量机的学习是在<strong>特征空间</strong>中进行的，线性可分SVM将输入空间中的输入线性映射为特征空间中的特征向量，非线性可分SVM将输入非线性映射为特征向量</p>
<ul>
<li>算法思想<ul>
<li>利用<strong>间隔最大化</strong>求最优分离超平面，<strong>解是唯一的</strong></li>
<li>能够正确划分训练数据集并且 <strong>几何间隔</strong> 最大</li>
</ul>
</li>
</ul>
<p>学习到的分离超平面：</p>
<script type="math/tex; mode=display">w^* * x + b^* = 0</script><p>最终的分离决策函数使用符号函数</p>
<h4 id="6-2-1-函数间隔和几何间隔"><a href="#6-2-1-函数间隔和几何间隔" class="headerlink" title="6.2.1 函数间隔和几何间隔"></a>6.2.1 函数间隔和几何间隔</h4><ul>
<li>函数间隔</li>
</ul>
<script type="math/tex; mode=display">\hat{\gamma}_i = y_i(w*x_i+b)</script><p>其中<script type="math/tex">y_i</script>用来表示分类预测的正确性, <script type="math/tex">w*x_i+b</script> 用来表示分类的确信度（换言之就是点到分类超平面的距离）</p>
<p>定义超平面的函数间隔为数据集D中所有样本的函数间隔中的最小值，即：</p>
<script type="math/tex; mode=display">\hat{\gamma} = \min_{i=1,...,N}\hat{\gamma}_i</script><ul>
<li>几何间隔</li>
</ul>
<p>由于成倍缩放<script type="math/tex">w,b</script>会导致函数间隔成倍缩放，所以 <script type="math/tex">w</script> 加上约束，如约束为：<script type="math/tex">\Vert{w}\Vert = 1</script>， 可以导出几何间隔</p>
<ul>
<li>函数间隔与几何间隔的关系</li>
</ul>
<script type="math/tex; mode=display">\gamma = \frac{\hat{\gamma}}{\Vert{w}\Vert}</script><h4 id="6-2-2-间隔最大化"><a href="#6-2-2-间隔最大化" class="headerlink" title="6.2.2 间隔最大化"></a>6.2.2 间隔最大化</h4><ul>
<li>直观解释<ul>
<li>使得分离有足够的确信度 [ 使得（离分离超平面 <strong>最近</strong> 的数据点）离超平面 <strong>足够远</strong> ]</li>
</ul>
</li>
</ul>
<p>可以得到求解式：</p>
<script type="math/tex; mode=display">\max{\frac{\hat{\gamma}}{\Vert{w}\Vert}}_{w,b}</script><script type="math/tex; mode=display">s.t. y_i(w*x_i+b) \geq \hat{\gamma}, i = i,2,...,N</script><p>令 <script type="math/tex">\hat{\gamma} = 1</script>,得到 <strong>线性可分支持向量机的最优化问题(凸二次优化问题)</strong>, 具体推导见《方法》P97：</p>
<script type="math/tex; mode=display">\min_{w,b} \frac{1}{2}{\Vert{w}\Vert}^2</script><script type="math/tex; mode=display">s.t. y_i(w*x_x+b) - 1 \geq 0, i=1,2,...,N</script><h4 id="6-2-3-学习的对偶算法"><a href="#6-2-3-学习的对偶算法" class="headerlink" title="6.2.3 学习的对偶算法"></a>6.2.3 学习的对偶算法</h4><p>求解 <strong>线性可分支持向量机的最优化问题</strong> 使用 <strong>拉格朗日对偶性</strong>，令原式为原始问题，通过求解对偶问题得到原始问题的最优解</p>
<ul>
<li>优点<ul>
<li>对偶问题更容易求解</li>
<li>自然引入核函数（在对偶式中出现了<strong>内积</strong>）</li>
</ul>
</li>
</ul>
<p>推导过程见 《统计学习方法》 P103</p>
<p>其中几个重要的点：</p>
<ul>
<li>支持向量是 <script type="math/tex">{\alpha_i}^* > 0</script> 的点</li>
<li>对偶学习问题的KKT条件</li>
</ul>
<p><img src="https://danielblog.oss-cn-beijing.aliyuncs.com/ML_Note/KKT.jpg" alt=""></p>
<ul>
<li>KKT的对偶互补条件</li>
</ul>
<script type="math/tex; mode=display">{\alpha}^* \geq 0</script><ul>
<li>实际应用中b的求解<ul>
<li>理论上使用任意一个数据<script type="math/tex">x_j</script>就能算出 <script type="math/tex">b</script></li>
<li>在实际应用中，采用更加鲁棒的做法:使用所有 <strong>支持向量</strong> 求解的平均值,<script type="math/tex">S</script>代表支持向量的集合</li>
</ul>
</li>
</ul>
<script type="math/tex; mode=display">b = \frac{1}{S}\sum_{s\in{S}}(\frac{1}{y_s} - \sum_{i\in{S}}\alpha_iy_ix_i^Tx_s)</script><h3 id="6-3-线性支持向量机与软间隔最大化"><a href="#6-3-线性支持向量机与软间隔最大化" class="headerlink" title="6.3 线性支持向量机与软间隔最大化"></a>6.3 线性支持向量机与软间隔最大化</h3><h4 id="6-3-1-线性支持向量机"><a href="#6-3-1-线性支持向量机" class="headerlink" title="6.3.1 线性支持向量机"></a>6.3.1 线性支持向量机</h4><ul>
<li><p>引入</p>
<ul>
<li>大部分数据并不满足完全线性可分，存在一部分数据非线性可分，因此对每一个数据引入一个<strong>松弛变量</strong> <script type="math/tex">\xi \geq 0</script>,使得那些不满足线性可分的数据点加上 <script type="math/tex">\xi</script> 变得线性可分</li>
</ul>
</li>
<li><p>线性支持向量机的原始问题：</p>
</li>
</ul>
<script type="math/tex; mode=display">\min_{w,b,\xi} \frac{1}{2}{\Vert{w}\Vert}^2+C\sum_{i=1}^N{\xi}_i</script><script type="math/tex; mode=display">s.t. y_i(w*x_i+b) \geq 1 - {\xi}_i, i = 1,2,...N</script><script type="math/tex; mode=display">{\xi}_i \geq 0, i=1,2,...N</script><ul>
<li><p>原始问题的对偶问题</p>
<ul>
<li>同线性可分的推导相同，先求<script type="math/tex">\min_{w,b,{\xi}}</script>(即分别对<script type="math/tex">w,b,\xi</script>求导，并令导数等于零，求出相应的表达式，代回原式，得到<script type="math/tex">\min_{w,b,{\xi}}L(w,b,\xi,\alpha,\mu)</script>)</li>
<li>再求出<script type="math/tex">\max_{\alpha}\min_{w,b,{\xi}}L(w,b,\xi,\alpha,\mu)</script></li>
</ul>
</li>
<li><p>原始问题满足的KKT条件</p>
</li>
</ul>
<p><img src="https://danielblog.oss-cn-beijing.aliyuncs.com/ML_Note/KKT_2.png" alt=""></p>
<h4 id="6-3-2-支持向量"><a href="#6-3-2-支持向量" class="headerlink" title="6.3.2 支持向量"></a>6.3.2 支持向量</h4><ul>
<li>若<script type="math/tex">{\alpha_i}^* <  C</script>, 则<script type="math/tex">\xi_i = 0</script>,支持向量恰好落在间隔边界上</li>
<li>若<script type="math/tex">{\alpha_i}^* =  C</script>，<script type="math/tex">0<\xi_i<1</script>,则此时分类正确，<script type="math/tex">x_i</script> 在分类超平面与间隔边界之间</li>
<li>若<script type="math/tex">{\alpha_i}^* =  C</script>，<script type="math/tex">\xi_i = 1</script>，则 <script type="math/tex">x_i</script> 在分离超平面上</li>
<li>若<script type="math/tex">{\alpha_i}^* =  C</script>，<script type="math/tex">\xi_i > 1</script>, 则 <script type="math/tex">x_i</script> 位于分类错误的一侧</li>
</ul>
<h4 id="6-3-3-合页损失函数"><a href="#6-3-3-合页损失函数" class="headerlink" title="6.3.3 合页损失函数"></a>6.3.3 合页损失函数</h4><ul>
<li>定义</li>
</ul>
<p><img src="https://danielblog.oss-cn-beijing.aliyuncs.com/ML_Note/hinge_loss_def.png" alt=""></p>
<ul>
<li>原始问题也等价于含有合页函数作为损失函数的目标函数</li>
<li>也即将 <script type="math/tex">\xi_i</script> 换成了<script type="math/tex">{[1-y_i(w*x_i+b)]}_+</script></li>
</ul>
<script type="math/tex; mode=display">{\sum_{i=1}^N}{[1 - y_i(w*x_i+b)]}_+ + \lambda{\Vert{w}\Vert}^2</script><ul>
<li>相比感知机的损失函数和0-1损失函数，合页损失函数有更高的要求</li>
</ul>
<p><img src="https://danielblog.oss-cn-beijing.aliyuncs.com/ML_Note/%E5%90%88%E9%A1%B5%E6%8D%9F%E5%A4%B1.png" alt=""></p>
<h3 id="6-4-支持向量回归"><a href="#6-4-支持向量回归" class="headerlink" title="6.4 支持向量回归"></a>6.4 支持向量回归</h3><ul>
<li>思想<ul>
<li>不同于传统回归模型，SVR对偏差有一定的容忍，形成一个以容忍度/宽度<script type="math/tex">2\epsilon</script>为间隔带，误差不超过<script type="math/tex">\epsilon</script>的样本都认为预测正确</li>
<li>SVR也可以引入核函数</li>
<li>SVR引入了 <script type="math/tex">\epsilon-不敏感损失</script></li>
</ul>
</li>
</ul>
<script type="math/tex; mode=display">
l_\epsilon(z) =
\begin{cases}
0,  & \text{if $|z| \leq \epsilon$} \\[2ex]
|z| - \epsilon, & \text{otherwise}
\end{cases}</script><ul>
<li>原始问题<ul>
<li>SVR考虑预测模型两侧的情况，所以约束条件分为两个</li>
</ul>
</li>
</ul>
<script type="math/tex; mode=display">f(x_i) - y_i \leq \epsilon + \xi_i, 上侧</script><script type="math/tex; mode=display">y_i - f(x_i) \leq \epsilon + \hat{\xi_i}, 下侧</script><h3 id="6-5-核函数"><a href="#6-5-核函数" class="headerlink" title="6.5 核函数"></a>6.5 核函数</h3><h4 id="6-5-1-核技巧"><a href="#6-5-1-核技巧" class="headerlink" title="6.5.1 核技巧"></a>6.5.1 核技巧</h4><p><strong>非线性分类问题</strong></p>
<ul>
<li>对于不能线性分类的数据，也就不能用一个超平面将数据分开，但如果能用一个 <strong>超曲面</strong> 将数据集分开，那么这个问题就是一个非线性可分问题</li>
<li>注意：是 <strong>非线性的</strong> 但是是 <strong>可分的问题</strong></li>
</ul>
<p><strong>求解思想</strong></p>
<ul>
<li>使用一个变换将原空间的数据映射到新空间，而这个新空间也就是 <strong>特征空间</strong></li>
<li>在 <strong>新空间\特征空间</strong> 上用 <strong>线性分类的学习方法</strong> 学习分类模型</li>
</ul>
<p><strong>核函数定义</strong></p>
<script type="math/tex; mode=display">K(x,z) = \phi(x)*\phi(z)</script><p>其中，<script type="math/tex">\phi{x}</script> 为 <strong>映射函数</strong></p>
<p><strong>表示定理</strong></p>
<ul>
<li>对于一般的损失函数和正则化项，优化问题都能表示为核函数的线性组合</li>
</ul>
<script type="math/tex; mode=display">h^*(x) = \sum_{i=1}^m\alpha_ik(x,x_i)</script><p><strong>核函数在SVM中的应用</strong></p>
<ul>
<li>将SVM对偶问题中的内积 <script type="math/tex">x_i*x_j</script> 变换为特征空间中的内积 <script type="math/tex">\phi(x_i)*\phi(x_j)</script></li>
<li>当映射函数是 <strong>非线性函数</strong> 时，学习到的核函数SVM是 <strong>非线性分类模型</strong></li>
</ul>
<p>使用核函数的支持向量机最优化问题</p>
<h3 id="6-6-习题思考"><a href="#6-6-习题思考" class="headerlink" title="6.6 习题思考"></a>6.6 习题思考</h3><ul>
<li><strong>为什么高斯核会把原始维度映射到无穷多维？</strong><ul>
<li>定义多项式核函数 <script type="math/tex">k(x,y) = {(x^Ty)}^p</script></li>
<li>然后考虑高斯核函数<script type="math/tex">k(x,y) = exp(-{\Vert{x-y}\Vert}^2)</script></li>
<li>展开后得到： <script type="math/tex">k(x,y) = exp(-{\Vert{x}\Vert}^2)exp(-{\Vert{y}\Vert}^2)exp(2xy)</script></li>
<li>由泰勒展开公式：</li>
</ul>
</li>
</ul>
<script type="math/tex; mode=display">指数函数： e^x = \sum_{n=0}^\infty{\frac{x^n}{n!}}</script><p>可以得到：</p>
<script type="math/tex; mode=display">k(x,y) =  exp(-{\Vert{x}\Vert}^2)exp(-{\Vert{y}\Vert}^2){\sum_{n=0}^\infty}{\frac{(2x^Ty)^n}{n!}}</script><p>可以看出，高斯核其实被展开为无穷项多项式核函数的和，而这其中也包括了无限维的多项式核，因此高斯核会把原始维度映射到无穷多维</p>
<h2 id="第七章-贝叶斯分类器"><a href="#第七章-贝叶斯分类器" class="headerlink" title="第七章 贝叶斯分类器"></a>第七章 贝叶斯分类器</h2><h3 id="7-1-贝叶斯决策论"><a href="#7-1-贝叶斯决策论" class="headerlink" title="7.1 贝叶斯决策论"></a>7.1 贝叶斯决策论</h3><p><strong>概念</strong></p>
<ul>
<li>贝叶斯决策论是建立在 <strong>概率</strong> 和 <strong>损失</strong> 之上的,以概率为基石辅以损失函数</li>
</ul>
<p>二者结合形成了在样本上的 <strong>“条件风险”</strong> 函数</p>
<script type="math/tex; mode=display">R(c_i|x) = \sum_{j=1}^N\lambda_{ij}P(c_j|x)</script><p>$\lambda_{ij}$ 是将真实标记为 $c_j$ 的标记预测为 $c_i$ 的损失，可以为任意的损失函数</p>
<p>$P(c_i|x)$ 是 <strong>可从数据中获得</strong> 样本分类为 $c_i$ 的 <strong>后验概率</strong></p>
<p>我们需要找到一个判定准则 <script type="math/tex">h</script> , 来 <strong>最小化总体风险</strong>, 即 使得预测结果类 <script type="math/tex">c^*</script> 是使得上面提到的损失最小的类 <script type="math/tex">c</script></p>
<p>因此最优的判定准则为：</p>
<script type="math/tex; mode=display">h(x^*) = \mathop{argmin}_{c\in{Y}}R(c|x)</script><p>现在，就我们就只需要考虑如何求得所有的 <script type="math/tex">R(c|x)</script>, 当损失函数是 <strong>0-1损失</strong> 时有：</p>
<script type="math/tex; mode=display">R(c|x) = 1 - P(c|x)</script><p><strong>注意！ 上式只在 损失函数 是 0-1损失 时成立！！！</strong></p>
<p>于是，我们可以得到<strong>贝叶斯最优分类器</strong>：</p>
<script type="math/tex; mode=display">h^*(x) = \mathop{argmax}_{c\in{Y}}P(c|x)</script><p><strong>后验概率的求解</strong></p>
<p>现在的任务变为了求解 <script type="math/tex">P(c|x)</script>, 主要有 <strong>判别式模型</strong> 和 <strong>生成式模型</strong></p>
<ul>
<li>生成式模型<ul>
<li>通过直接建立模型来预测 c</li>
<li>代表：决策树、BP神经网络、支持向量机</li>
</ul>
</li>
</ul>
<ul>
<li>判别式模型<ul>
<li>通过对概率分布 <script type="math/tex">P(c|x)</script> 建模</li>
<li>代表： 贝叶斯模型</li>
<li>求解公式,基于贝叶斯公式：</li>
</ul>
</li>
</ul>
<script type="math/tex; mode=display">P(c|x) = \frac{P(c)P(x|c)}{P(x)}</script><p>而对于所有的训练数据，<script type="math/tex">P(x)</script> 是相同的，不影响求解最优，由此，贝叶斯最优分类器的公式可以更改为：</p>
<script type="math/tex; mode=display">h^*(x) = \mathop{argmax}_{c\in{Y}}P(c)P(x|c)</script><h3 id="7-2-朴素贝叶斯分类器"><a href="#7-2-朴素贝叶斯分类器" class="headerlink" title="7.2 朴素贝叶斯分类器"></a>7.2 朴素贝叶斯分类器</h3><p>由于通常数据的特征/维度会有很多，同时每一个特征的取值也有很多，例如一共有K个特征，而每一个特征是一个二值特征，那么整个数据空间将会达到 $2^K$ 的规模，因此与测试时很有可能该数据在训练样本中没有出现过，导致 $P(x|c) = 0$，这显然不对，没有出现过并不等价于概率为零</p>
<p>因此引入 <strong>朴素贝叶斯分类器</strong></p>
<p>而在朴素贝叶斯方法中，<strong>学习</strong> 指的就是 $P(c)$ 和 $P(x|c)$ 的 <strong>估计</strong></p>
<h4 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h4><ul>
<li>假设每个属性 <strong>独立</strong> 的对分类结果产生影响</li>
</ul>
<script type="math/tex; mode=display">P(c|x)=\frac{P(c)P(x|c)}{P{x}}=\frac{P(c)}{P(x)}\prod_{i=1}^dP(x_i|c)</script><p>其中 $d$ 为属性数目， $x_i$ 为 $x$ 在第 i 个 属性上的 <strong>取值</strong></p>
<h4 id="计算"><a href="#计算" class="headerlink" title="计算"></a>计算</h4><ul>
<li>先验概率</li>
</ul>
<p>令 $D_c$ 表示训练集 D 中第 c 类样本组成的集合</p>
<script type="math/tex; mode=display">P(c) = \frac{|D_c|}{|D|}</script><ul>
<li>后验概率</li>
</ul>
<p>对于<strong>离散属性</strong>,令 $D_{c,x_i}$ 表示 $D_c$ 上在第 i 个属性上取值为 $x_i$ 的样本组成的集合</p>
<script type="math/tex; mode=display">P(x_i|c)=\frac{|D_{c,x_i}|}{D_c}</script><p>对于<strong>连续属性</strong>，考虑概率密度函数，假设服从正态分布 $N(\mu_{c,i},\sigma_{c,i}^2)$, 其中 $\mu_{c,i}$, $\sigma_{c,i}^2$表示第 c 类样本在第 i 个属性上取值的 均值 和 方差</p>
<script type="math/tex; mode=display">p(x_i|c)=\frac{1}{\sqrt{2\pi}\sigma_{c,i}}exp(-\frac{(x_i - \mu_{c,i})^2}{2\sigma_{c,i}^2})</script><p>上面三个式子就是基于 <strong>极大似然估计</strong> 的结果</p>
<h4 id="贝叶斯估计-平滑"><a href="#贝叶斯估计-平滑" class="headerlink" title="贝叶斯估计\平滑"></a>贝叶斯估计\平滑</h4><p>贝叶斯估计就是加上 <strong>平滑</strong> 的极大似然估计</p>
<p><strong>目的</strong></p>
<p>为了避免某些属性值在计算时没有在训练数据中出现，导致概率为零最后导致连乘最后的结果为零，对上面离散情况的计算式子进行修正</p>
<script type="math/tex; mode=display">\hat{P}(c) = \fac{|D_c| + \lambda}{|D| + \lambda{N}}</script><script type="math/tex; mode=display">\hat{P}(x_i|c) = \frac{|D_{c,x_i}| + \lambda}{|D_c| + \lambda{N_i}}</script><p>常用 <strong>拉普拉斯修正</strong>, 即上式中 $\lambda = 1$</p>
<h2 id="第八章-EM算法"><a href="#第八章-EM算法" class="headerlink" title="第八章 EM算法"></a>第八章 EM算法</h2><h3 id="8-1-概念"><a href="#8-1-概念" class="headerlink" title="8.1 概念"></a>8.1 概念</h3><p>期望最⼤化算法，或者EM算法，是寻找具有 <strong>潜在变量</strong> 的概率模型的 <strong>最⼤似然</strong> 解的⼀种通⽤的⽅法（PRML）</p>
<p>但是当 $Z$ 是连续变量或者离散变量与连续变量的组合时，⽅法是完全相同的，只需把求和换成适当的<strong>积分</strong>即可（PRML）</p>
<ul>
<li>为什么EM算法需要先给定一个先验的参数 $\theta$？</li>
</ul>
<p>因为在求解最大化对数似然 $log\sum_z{P(Y,Z|\theta)}$ 的过程中引入了一个 <strong>常数</strong>，而这个常数是一个已知了参数 $\theta$ 和$Y$的关于隐变量 $z$ 的概率值，因为需要提前已知参数 $\theta$ 分布函数才是确定的，然后带入 $Y$ 才能利用分布函数求出当前隐变量 $z$ 的概率值</p>
<ul>
<li>每次 M步 最大化的是什么？</li>
</ul>
<p>最大化的是期望，而这个期望是<strong>对数似然的期望</strong>，其中对数似然的分布函数已经是给定了的（其中分布函数是有关\theta的）</p>
<ul>
<li>每次 E步 求的是什么？</li>
</ul>
<p>求的是 M步最大化需要用到的提前给定的 <strong>对数似然的分布函数</strong>， 利用提前给的 $\theta$ 进行求解</p>
<h3 id="8-2-算法步骤"><a href="#8-2-算法步骤" class="headerlink" title="8.2 算法步骤"></a>8.2 算法步骤</h3><ul>
<li>输入模型参数的初值 $\theta^{(0)}$</li>
<li>E步： 利用已知的参数$\theta^i$ 计算函数 $Q(\theta,\theta^{(i)}) = \sum_zlogP(Z|Y,\theta^{(i)})P(Y,Z|\theta)$<ul>
<li>其中函数 $Q(\theta,\theta^{(i)})$ 就代表<strong>对数似然的期望</strong></li>
<li>需要计算的就是 $P(Z|Y,\theta^{(i)})$</li>
<li>其中含有 M步需要最大化的参数$\theta$</li>
</ul>
</li>
<li>M步： 最大化E步求出的 $Q(\theta,\theta^{(i)})$ 中的参数 $\theta$</li>
<li>将 M步 求出的最大的 $\theta$ 作为E步中已知的参数$\theta^i$，迭代进行计算，直到参数 $\theta$ 收敛（变化很小）</li>
</ul>
<h3 id="8-3-算法是怎么来的-坐标上升"><a href="#8-3-算法是怎么来的-坐标上升" class="headerlink" title="8.3 算法是怎么来的 + 坐标上升"></a>8.3 算法是怎么来的 + 坐标上升</h3><ul>
<li><p>思想</p>
<ul>
<li>最大化观测数据的对数似然函数 $logP(Y|\theta)$</li>
<li>找到一个对数似然函数的下界函数 $Q(\theta)$</li>
<li>找到下界函数 $Q(Z)$ 的最大值 $\theta^*$</li>
<li>下界函数利用 <strong>Jensen不等式</strong> 得到（提示：log函数是一个凹函数）求得，其中需要引入一个使用 $\theta^{(i)}$ 的先验分布 $Q(z^{(i)})$</li>
</ul>
</li>
<li><p>如何求得需要引入的先验分布 $Q(z^{(i)})$？</p>
<ul>
<li>从 Jensen不等式入手，由于Jensen 表示： $E[f(x)] \leq f[E(x)]$, 当$x$为常数c的时候，左右式相等，此时取到等号</li>
<li>此时求得 $Q(z^{(i)}) = P(z^{(i)}|Y,\theta)$</li>
</ul>
</li>
</ul>
<p>EM算法是一个迭代计算的算法，思想就是每次固定一个变量</p>
<ul>
<li>首先固定住 $\theta$, 调整先验分布函数$Q(z^i)$取到等号</li>
<li>然后固定住先验分布函数$Q(z^{(i)})$，调整$\theta$，使先验分布函数$Q(z^{(i)})$取得最大值，得到$\theta^{new}$ </li>
<li>再固定住$\theta$，此时使用刚计算出的$\theta^{new}$ ，调整先验分布函数$Q(z^i)$取到等号….</li>
<li>直到对数似然函数收敛到最大值$\theta^*$ </li>
</ul>
<p>因此，EM算法可以看做利用 <strong>坐标上升</strong> 进行优化的算法</p>
<p><img src="https://danielblog.oss-cn-beijing.aliyuncs.com/ML_Note/EM.png" alt=""></p>

      
    </div>
    
    
    

	<div>
      
        
<div class="my_post_copyright">
  <script src="//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js"></script>
  <!-- JS库 sweetalert 可修改路径 -->
  <script type="text/javascript" src="http://jslibs.wuxubj.cn/sweetalert_mini/jquery-1.7.1.min.js"></script>
  <script src="http://jslibs.wuxubj.cn/sweetalert_mini/sweetalert.min.js"></script>
  <link rel="stylesheet" type="text/css" href="http://jslibs.wuxubj.cn/sweetalert_mini/sweetalert.mini.css">
  <p><span>本文标题:</span><a href="/2018/05/22/机器学习笔记/">西瓜书机器学习笔记</a></p>
  <p><span>文章作者:</span><a href="/" title="访问 Daniel_柏桦 的个人博客">Daniel_柏桦</a></p>
  <p><span>发布时间:</span>2018年05月22日 - 20:05</p>
  <p><span>最后更新:</span>2018年07月22日 - 23:07</p>
  <p><span>原始链接:</span><a href="/2018/05/22/机器学习笔记/" title="西瓜书机器学习笔记">https://danielack.github.io/2018/05/22/机器学习笔记/</a>
    <span class="copy-path"  title="点击复制文章链接"><i class="fa fa-clipboard" data-clipboard-text="https://danielack.github.io/2018/05/22/机器学习笔记/"  aria-label="复制成功！"></i></span>
  </p>
  <p><span>许可协议:</span><i class="fa fa-creative-commons"></i> <a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" title="Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)">署名-非商业性使用-禁止演绎 4.0 国际</a> 转载请保留原文链接及作者。</p>  
</div>
<script> 
    var clipboard = new Clipboard('.fa-clipboard');
    clipboard.on('success', $(function(){
      $(".fa-clipboard").click(function(){
        swal({   
          title: "",   
          text: '复制成功',   
          html: false,
          timer: 500,   
          showConfirmButton: false
        });
      });
    }));  
</script>

      
	</div>
    

    
      <div>
        <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
  <div>坚持原创技术分享，您的支持将鼓励我继续创作！</div>
  <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
    <span>打赏</span>
  </button>
  <div id="QR" style="display: none;">

    
      <div id="wechat" style="display: inline-block">
        <img id="wechat_qr" src="/images/wechatpay.jpg" alt="Daniel_柏桦 微信支付"/>
        <p>微信支付</p>
      </div>
    

    
      <div id="alipay" style="display: inline-block">
        <img id="alipay_qr" src="/images/alipay.jpg" alt="Daniel_柏桦 支付宝"/>
        <p>支付宝</p>
      </div>
    

    

  </div>
</div>

      </div>
    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/机器学习/" rel="tag"><i class="fa fa-tag"></i> 机器学习</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/05/20/Logistic Regression的理解/" rel="next" title="关于Logistic Regression的理解">
                <i class="fa fa-chevron-left"></i> 关于Logistic Regression的理解
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      
        <div id="gitment-container"></div>
      
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/head.jpg"
                alt="Daniel_柏桦" />
            
              <p class="site-author-name" itemprop="name">Daniel_柏桦</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">6</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">9</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/danielAck" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="danieljr@126.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://weibo.com/u/3946613575" target="_blank" title="微博">
                      
                        <i class="fa fa-fw fa-weibo"></i>微博</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#第一章-绪论"><span class="nav-number">1.</span> <span class="nav-text">第一章 绪论</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-基本术语"><span class="nav-number">1.1.</span> <span class="nav-text">1.2 基本术语</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#第二章-模型评估与选择"><span class="nav-number">2.</span> <span class="nav-text">第二章 模型评估与选择</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-经验误差与过拟合"><span class="nav-number">2.1.</span> <span class="nav-text">2.1 经验误差与过拟合</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-评估方法"><span class="nav-number">2.2.</span> <span class="nav-text">2.2 评估方法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#测试集的划分方法"><span class="nav-number">2.2.1.</span> <span class="nav-text">测试集的划分方法</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-性能度量"><span class="nav-number">2.3.</span> <span class="nav-text">2.3 性能度量</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-3-1-错误率与精度"><span class="nav-number">2.3.1.</span> <span class="nav-text">2.3.1 错误率与精度</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-3-2-查准率、查全率与-F1"><span class="nav-number">2.3.2.</span> <span class="nav-text">2.3.2 查准率、查全率与 F1</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-3-3-ROC与AUC"><span class="nav-number">2.3.3.</span> <span class="nav-text">2.3.3 ROC与AUC</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-3-4-代价敏感错误率与代价曲线"><span class="nav-number">2.3.4.</span> <span class="nav-text">2.3.4 代价敏感错误率与代价曲线</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-比较检验"><span class="nav-number">2.4.</span> <span class="nav-text">2.4 比较检验</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-5-偏差与方差"><span class="nav-number">2.5.</span> <span class="nav-text">2.5 偏差与方差</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#第3章-线性模型"><span class="nav-number">3.</span> <span class="nav-text">第3章 线性模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-基本形式"><span class="nav-number">3.1.</span> <span class="nav-text">3.1 基本形式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-线性回归"><span class="nav-number">3.2.</span> <span class="nav-text">3.2 线性回归</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-对数几率回归"><span class="nav-number">3.3.</span> <span class="nav-text">3.3 对数几率回归</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-4-线性判别分析"><span class="nav-number">3.4.</span> <span class="nav-text">3.4 线性判别分析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-5-多分类学习"><span class="nav-number">3.5.</span> <span class="nav-text">3.5 多分类学习</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#多对多分类"><span class="nav-number">3.5.1.</span> <span class="nav-text">多对多分类</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-6-类别不平衡问题"><span class="nav-number">3.6.</span> <span class="nav-text">3.6 类别不平衡问题</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#“再缩放”-思想"><span class="nav-number">3.6.1.</span> <span class="nav-text">“再缩放” 思想</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#欠采样、过采样思想"><span class="nav-number">3.6.2.</span> <span class="nav-text">欠采样、过采样思想</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-7-相关思考"><span class="nav-number">3.7.</span> <span class="nav-text">3.7 相关思考</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#第4章-决策树"><span class="nav-number">4.</span> <span class="nav-text">第4章 决策树</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-决策树模型与学习"><span class="nav-number">4.1.</span> <span class="nav-text">4.1 决策树模型与学习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-特征选择（划分选择）"><span class="nav-number">4.2.</span> <span class="nav-text">4.2 特征选择（划分选择）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3-决策树的生成"><span class="nav-number">4.3.</span> <span class="nav-text">4.3 决策树的生成</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-4-决策树的剪枝"><span class="nav-number">4.4.</span> <span class="nav-text">4.4 决策树的剪枝</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-5-连续与缺失值"><span class="nav-number">4.5.</span> <span class="nav-text">4.5 连续与缺失值</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#4-5-1-连续值的处理"><span class="nav-number">4.5.1.</span> <span class="nav-text">4.5.1 连续值的处理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-5-2-缺失值的处理"><span class="nav-number">4.5.2.</span> <span class="nav-text">4.5.2 缺失值的处理</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-6-多变量决策树"><span class="nav-number">4.6.</span> <span class="nav-text">4.6 多变量决策树</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-7-习题思考"><span class="nav-number">4.7.</span> <span class="nav-text">4.7 习题思考</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#第5章-神经网络"><span class="nav-number">5.</span> <span class="nav-text">第5章 神经网络</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#第6章-支持向量机"><span class="nav-number">6.</span> <span class="nav-text">第6章 支持向量机</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#6-1-概念"><span class="nav-number">6.1.</span> <span class="nav-text">6.1 概念</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-2-线性可分支持向量机"><span class="nav-number">6.2.</span> <span class="nav-text">6.2 线性可分支持向量机</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#6-2-1-函数间隔和几何间隔"><span class="nav-number">6.2.1.</span> <span class="nav-text">6.2.1 函数间隔和几何间隔</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-2-2-间隔最大化"><span class="nav-number">6.2.2.</span> <span class="nav-text">6.2.2 间隔最大化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-2-3-学习的对偶算法"><span class="nav-number">6.2.3.</span> <span class="nav-text">6.2.3 学习的对偶算法</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-3-线性支持向量机与软间隔最大化"><span class="nav-number">6.3.</span> <span class="nav-text">6.3 线性支持向量机与软间隔最大化</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#6-3-1-线性支持向量机"><span class="nav-number">6.3.1.</span> <span class="nav-text">6.3.1 线性支持向量机</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-3-2-支持向量"><span class="nav-number">6.3.2.</span> <span class="nav-text">6.3.2 支持向量</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-3-3-合页损失函数"><span class="nav-number">6.3.3.</span> <span class="nav-text">6.3.3 合页损失函数</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-4-支持向量回归"><span class="nav-number">6.4.</span> <span class="nav-text">6.4 支持向量回归</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-5-核函数"><span class="nav-number">6.5.</span> <span class="nav-text">6.5 核函数</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#6-5-1-核技巧"><span class="nav-number">6.5.1.</span> <span class="nav-text">6.5.1 核技巧</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-6-习题思考"><span class="nav-number">6.6.</span> <span class="nav-text">6.6 习题思考</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#第七章-贝叶斯分类器"><span class="nav-number">7.</span> <span class="nav-text">第七章 贝叶斯分类器</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#7-1-贝叶斯决策论"><span class="nav-number">7.1.</span> <span class="nav-text">7.1 贝叶斯决策论</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-2-朴素贝叶斯分类器"><span class="nav-number">7.2.</span> <span class="nav-text">7.2 朴素贝叶斯分类器</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#概念"><span class="nav-number">7.2.1.</span> <span class="nav-text">概念</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#计算"><span class="nav-number">7.2.2.</span> <span class="nav-text">计算</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#贝叶斯估计-平滑"><span class="nav-number">7.2.3.</span> <span class="nav-text">贝叶斯估计\平滑</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#第八章-EM算法"><span class="nav-number">8.</span> <span class="nav-text">第八章 EM算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#8-1-概念"><span class="nav-number">8.1.</span> <span class="nav-text">8.1 概念</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-2-算法步骤"><span class="nav-number">8.2.</span> <span class="nav-text">8.2 算法步骤</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-3-算法是怎么来的-坐标上升"><span class="nav-number">8.3.</span> <span class="nav-text">8.3 算法是怎么来的 + 坐标上升</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Daniel_柏桦</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  







<!-- LOCAL: You can save these files to your site and update links -->
    
        
        <link rel="stylesheet" href="https://aimingoo.github.io/gitmint/style/default.css">
        <script src="https://aimingoo.github.io/gitmint/dist/gitmint.browser.js"></script>
    
<!-- END LOCAL -->

    

    
      <script type="text/javascript">
      function renderGitment(){
        var gitment = new Gitmint({
            id: '1526992035000', 
            owner: 'danielAck',
            repo: 'danielAck.github.io',
            
            lang: "" || navigator.language || navigator.systemLanguage || navigator.userLanguage,
            
            oauth: {
            
            
                client_secret: 'cfb3be5fb1304cb6074d91f1371b5e77668d68ba',
            
                client_id: 'beb9662dd9e1835fe45b'
            }});
        gitment.render('gitment-container');
      }

      
      renderGitment();
      
      </script>
    







  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("GPlqVvbroSWT8qwXkqcF6mhN-gzGzoHsz", "lzTdjGEJmEHVdPpoAgmAkg5t");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

  <!-- 页面点击小红心 -->
  <script type="text/javascript" src="/js/src/love.js"></script>

</body>
</html>
