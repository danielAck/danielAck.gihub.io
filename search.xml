<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[西瓜书机器学习笔记]]></title>
    <url>%2F2018%2F05%2F22%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[对西瓜书的知识点进行一个梳理 第一章 绪论1.2 基本术语 所有的可能的模型组合成为一个假设空间 H 预测分为两类任务： 分类任务： 预测的结果是离散值 二分类任务，其中标签为(0/1 或者 +1/-1 代表 正例/负例) 多分类任务，类别数目 k &gt; 2 回归任务： 预测的结果是连续值 同时另一种任务是 聚类：事先不知道数据的规律，同时没有标签，属于无监督学习 建立模型/学习 的目的 找到一个从输入空间 X -&gt; 输出 Y的函数映射 使得学习到的模型能够很好的拟合“新数据”，即具有很好的泛化能力 通常假设样本数据服从独立同分布（i.i.d），而服从的分布是未知的，我们需要做的就是找出这个分布规律 归纳偏好 可以理解为对不同特征的重视程度 任何一个有效的机器学习算法必有相应的归纳偏好，因为能够拟合有限数据的曲线并不唯一，因此需要设定归纳偏好，使得泛化能力得到保证 通常使用 奥卡姆剃刀 原则作为一般性的归纳偏好原则，但是很遗憾原则中 “简单” 的含义针对不同的模型并不唯一，需要借助其他机制才能完成判断 NFL (No Free Lunch Theorem) 指出，必须结合具体的学习问题去讨论算法的优劣性 需要注意学习算法自身的偏好与问题是否匹配，不同的数据集有不同的“特点”，需要不同的“偏好”去拟合具体的数据集 第二章 模型评估与选择2.1 经验误差与过拟合 误差分为两类： 经验误差： 数据集上的误差 泛化误差： 新样本上的误差 过拟合的理解： 学习到了数据集中不够 general 的特性 把训练集自身的某些特性当做整体的特性 过拟合是无法避免的 2.2 评估方法 模型选择中存在两个问题： 泛化误差我们无从知晓 训练误差不靠谱(存在过拟合的情况） 因此提出不同的评估方法 需要一个 “测试集” ，通过 “测试集” 上的测试误差作为泛化误差的估计 测试集尽量不在训练集中出现，即二者尽量互斥 测试集的划分方法 留出法 将数据集划分为两个互斥的集合，同时保留类别比例(即进行分层抽样) 一般采用确定分层比例后，在层的大小内进行若干次随机划分、重复进行后取平均值作为最终结果 交叉验证法 与留出法相似，将数据集A分为 k 个互斥的集合 1234567for i in k: 将 A - k_i 作为训练集 将 k_i 作为测试集 记录 测试误差 err_ireturn 测试误差的平均数 p次k折交叉验证：通常重复 p 次进行，取 p 次结果的均值 目的: 为了减少样本划分不同而引入的误差 留一法 k 折交叉验证法的特殊情况 当 k = m , 此时称为 留一法 自助法 在 数据集较小、很难划分 训练集/ 测试集 的时候很有效 但是，此方法产生的数据改变了出事数据集的分布，会引入估计偏差 具体方法： 假设数据集 D 有 m 个样本 每次从数据集 D 中采出一个样本，放入 D‘ 中，再将样本放回 D 重复进行 m 次采样操作 用 D’ 作为训练集， D / D{`} 作为测试集 可证明，测试集中度数据占 D 中的 1/3 ， 且均未出现过在 D‘ 中 自助法在集成学习中的 Bagging 模型中还会用到（抽样方法） 验证集 我们将数据集划分为训练集和验证集，在验证集上进行模型选择和调参 注意！！！在模型选择调节完毕之后，还应将整个数据集 D 再完整训练一遍！ 2.3 性能度量 不同的性能度量导致不同的评判结果 不能仅仅依赖评判结果，不同的任务需求和数据需要不同的模型和性能度量 2.3.1 错误率与精度 分类任务中最常用的性能度量，可用于 二分类 及 多分类 2.3.2 查准率、查全率与 F1 查准率 也叫 准确率（Precision）查全率 也叫 召回率（Recall） Precision 和 Recall 定义为： {P} = TP \over {TP + FP}{R} = TP \over {TP + FN} Precision 很高往往导致 Recall 较低，可以理解为谨小慎微，需要很大的确信度才会做决定 Recall 很高往往导致 Precision 较低，可以理解为宁可错杀一万，也不会放过一个 P-R 曲线 绘制具体步骤 参考ROC曲线的绘制过程，将预测结果排序，依次将预测结果设置为阈值然后计算相应的 Precision 和 Recall 若分类器的 A 的 P-R 曲线完全被另一个分类器B “包住” ,则 B 的性能优于 A 平衡点（BEP） 是当 Precision = Recall 时的取值，BEP 大的分类器更优 F1 Score 比算术平均、几何平均更加重视较小值 {F_1} = \frac {2 \times P \times R}{P + R}Fβ Score 针对对于查准率和查全率重视程度不一样的情况比算术平均、几何平均更加重视较小值 {F_\beta} = \frac {(1 + {\beta}^2) \times P \times R}{({\beta}^2 \times P) + R}macro F1 Score 亦称 宏F1直接计算，再求平均 {macro-P} = {1 \over n} \sum_{i=1}^{n}P_i同理可得 macro-R，利用 macro-P、macro-R 和 F1 计算公式最后可得macro-F1 micro F1 Score亦称 微F1，同理macro-F1，记为micro-F1，先对 TP、TN、FP、FN 求平均，再计利用平均值计算F1 Score 2.3.3 ROC与AUC ROC - 受试者工作特征 横轴： TPR ； 纵轴： FPR AUC - 曲线下面积 面积的大小代表预测的准确性 绘制步骤 对每个测试样例进行预测，得到预测结果 对预测结果进行排序 将分类阈值一次设置为每一个预测值即将每个样例设置为正例） 设置好阈值之后，循环其他样例，若样例为真正例，则纵坐标加1 \over m^+ AUC中坐标 x 代表 该点之前的反例所占比，因为每遇到一个反例横坐标扩大一个百分点（1 \over m^-），同理，y 代表 该点之前的正例所占比 AUC的范围是 [0.5, 1] 2.3.4 代价敏感错误率与代价曲线敏感代价错误率 E(f;D;cost) = \frac{1}{m} (\sum_{x \in {D^+}}I(f(x_i)\not=y_i) \times cost_{01} + \sum_{x_i \in D^-}I(f(x_i)\not=y_i) \times cost_{10}) 为了权衡不同类型的错误造成的代价 使用 代价矩阵(cost_{01}，cost_{10}...)刻画不同的代价 使用 比值 刻画代价之间的重要程度 2.4 比较检验 使用假设检验的方法，进行误差的假设检验 后期进行二项检验、t检验的具体笔记 2.5 偏差与方差 针对回归问题，期望泛化误差可以分解为 偏差、方差、噪声之和 分解结果： E(f;D) = bias^2(x) + var(x) + \epsilon^2 具体分解过程见书 P45页 偏差与方差的关系 偏差与方差是 准 与 确 的关系 偏差 ：刻画预测结果的 确 的程度，即算法本身的拟合能力 方差 ：刻画预测结果的 准 的程度，也提现算法抗数据扰动的能力 噪声 ： 刻画算法达到期望泛化误差的下界，即学习问题本身的难度 学习初期， 偏差 主导泛化错误率，体现为 欠拟合 学习后期，欠拟合问题逐渐解决， 方差 主导泛化错误率，体现为 过拟合 ，在学习器学习过程中，会渐渐学习到数据中的 干扰数据，从而导致学习器的抗干扰能力下降，也即方差增加 偏差与方差的经典关系图： Min-Max规范化和z-score规范化的区别 Min-Max z-score 计算量小 计算量大 易受极大/极小值影响 不易受极值影响 超出原极值范围才需要重新计算 来一个数据重新计算一次 第3章 线性模型3.1 基本形式 试图用 属性 的 线性组合 进行预测 基本形式： f(x) = \omega^Tx + b = \omega_1x_1 + \omega_2x_2 + \cdots + \omega_dx_d + b $\omega$ 代表了各属性的 重要性， 因此线性模型具有很好的 可解释性 3.2 线性回归回归模型的变量通常是连续值，因此当属性是离散值时，可以进行 连续化 当属性件存在 “序” 的关系，通过对离散值进行排序，然后转换为相应大小的连续值 当属性间不存在 “序” 的关系，可以通过使用 OneHot 编码方法进行离散值的转换 无序属性进行连续化会不恰当的引入 “序” 的关系 损失函数： 使用 均方误差(MSE) 进行性能度量 MSE = \sum_{i=1}^{m}{(f(x_i) - y_i)}^2 = \sum_{i=1}^{m}{(\omega x_i + b - y_i)}^2求解方法： 最小二乘法 - 找到一条直线使所有的样本到直线的欧氏距离最小 3.3 对数几率回归也叫逻辑回归（Logistic Regression），虽然叫回归，但是其实是 分类 算法 起源 希望使用 线性模型 进行 分类预测任务 因此输出结果需要是 离散的 类别标签，二分类为例： y = {0,1} 但是线性模型的预测结果是连续值，因此我们需要找到一个函数将连续的结果值映射为离散的类别标签 因此，单位阶跃函数 是理想的映射函数，但是单位阶跃函数并不单调可微，这样就引入了 对数几率函数 作为替代函数 预测的结果其实是 对率几率，定义如下： \omega x^T + b = ln \frac {y}{1-y} 而 y 才是概率 去除 ln 有 几率 的定义，反映 x 作为正例的相对可能性 直接对 分类可能性 建模，无需实现假设数据分布，避免假设分布不准确引入的问题 3.4 线性判别分析核心思想： 异类点的投影点尽可能的远离，同类点之间的协方差尽可能的小(即同类点之间尽可能的靠近) 优化目标 J = \frac{w^TS_bw}{w^TS_ww} 二维情况详细推导可以参考 https://www.cnblogs.com/engineerLF/p/5393119.html 其中可以用“广义瑞利商”的性质得出最优w ,详细性质推导见 https://www.cnblogs.com/pinard/p/6244265.html 最大化方法 添加负号，变为最小化问题 使用拉格朗日乘子法， 求解结果 在一般的实践中，S_w^{-1} 使用SVD进行奇异值分解，通过 S^{-1}_w = V{\Sigma}^{-1}U^T 得到 {S_w}^{-1} KLDA 在LDA中引入核函数，应对非线性可分问题，具体推导见西瓜书P137 3.5 多分类学习 三种拆分策略： 一对一（OvO） 一对其余（OvR） 多对多（MvM） 一对一 将类别两两配对生成 $\frac {N(N-2)}{2}$ 个分类器 一对其余 每次将一个类别视为正例，其余视为反例，因此一共会生成 $N$ 个分类器 但是，一对一的 训练开销 通常小于 一对其余，因为一对一只用到了两个类的训练数据 多对多分类 常用纠错输出码（ECOC） 常见编码矩阵：二元码，三元码（加上停用类，不带入计算） 将类别进行随机拆分，拆分为两类：正例和反例，形成很多个二分类器 任意两个分类器越 ”不相同“，训练结果越好，思想类似于 Bagging，训练多个高多样性的分类器 最后这些分类器对同一个样本预测，将预测结果分别计算编码距离（海明距离 / 欧式距离），将编码距离最小的类作为最终结果 同一个分类任务， ECOC编码越长，分类器之间的相似性就会越小，因此容错能力越好 同等长度的编码， 任意两个类别的编码距离越大，容错能力越好 3.6 类别不平衡问题“再缩放” 思想在对分类任务进行预测时，y 是代表正例的概率，通常将阈值设置为 0.5，即 $y &gt; 0.5$ ,将结果判定为正例，否则判定为负例 言外之意： 若 y > 0.5, 预测为正例即若 \frac{y}{1-y} > 1, 预测为正例但是，注意这样做的原因：0.5 代表的其实是 观测几率, 即我们在数据集中能观测到正例的几率（我们 假设 两个类别的数据集是一样大的，因此观测几率是 0.5）所以当 $y &gt; 0.5$ 代表预测正例的概率大于观测概率，当然应该判定为正例。 而现在，不同的类别对应的数据量不同了，不再是 1 ： 1，我们的阈值就不应该设置为 0.5， 而是应该当： 若 \frac {y}{1-y} > \frac{m^+}{m^-} ，预测为正例解出 y ，就是我们新的阈值 欠采样、过采样思想欠采样 减少数据集的数目，使得所有类别的数据相同 代表算法： EasyEnsemble，即分割负例为不同的数据集，以供多个学习器使用，利用了集成的思想 过采样 增加数据集的数目，使得所有类别的数据相同 需要注意增加数据集的方法，单纯的 重复采样 会导致严重的 过拟合 代表算法： SMOTE 第4章 决策树4.1 决策树模型与学习优点 模型具有可读性，分类速度快 概念 损失函数使用 正则化的极大似然函数 学习的策略是 最小化损失函数 使用启发式方法求解决策树，求解的结果是次优的 结点有两种类型：内部结点(非叶子结点）表示一个特征或属性， 叶子结点代表一个类 还表示给定特征条件下的概率分布 决策树的学习算法包括： 特征选择（划分选择） 决策树的生成，只考虑局部最优 决策树的剪枝，考虑全局最优 注意两种情况： 当属性集为空或所有样本在属性集上的取值相同，将类别标记为 D 中最多的类别，使用的是当前结点的 后验分布 当当前结点包含的样本集为空，同样将类别标记为 D 中最多的类别，但是是利用的当前结点的 先验分布 4.2 特征选择（划分选择）常用 3 种指标选择应作为划分的特征 信息增益 对 可取数目较多 的特征(属性)有所偏好 增益率 对 可取数目较少 的特征(属性)有所偏好 所以通常采用一种折中的方法： 先找出高于平均水平的 信息增益， 再找出其中 信息增益率 最大的特征(属性) 基尼系数 4.3 决策树的生成常用的 3 种生成算法： ID3， 使用 信息增益 进行特征选择 C4.5， 使用 增益率 进行特征选择 CART， 使用 基尼系数 进行特征选择 4.4 决策树的剪枝剪枝策略 预剪枝 后剪枝 预剪枝 在划分结点前估计，若划分结点后不能提升泛化性能，则停止划分 使用贪心的策略，有可能陷入局部最优 带来欠拟合的风险 后剪枝 等决策树完全生成好之后，再从下而上的进行剪枝 剪枝策略： 将结点的子树替换为叶节点，观察泛化性能是否提升，如果提升则将子树替换为叶节点，否则不进行剪枝 根据奥卡姆剃刀原则，如果泛化性能不变依然应该选择剪枝，形成更简单的决策树 可以较好的防止过拟合，但是时间开销很大 这里的泛化性能评估标准可以是 精度 4.5 连续与缺失值4.5.1 连续值的处理假设属性 A 为连续值属性， A 属性共有 N 个不同的值 将这 N 个不同的值进行排序 依次选取划分点 t，大于划分点 t 的记为 $D_t^+$, 同理，小于 t 的记为 $D_t^-$ 这样，可以将每一个取值都作为一次划分点，然后计算 信息增益, 最后可以得到属性 A 的信息增益 后续就跟离散值的做法一样了 4.5.2 缺失值的处理当计算属性 A 的信息增益时，如果发现有样本有缺失值： 初始将所有样本赋予 1 的 权值 计算 属性a 上的三个概率（主要都是针对不含有缺失值的样本） 无缺失样本所占总体数据比例 无缺失样本中第k类所占比例 属性 a 上无缺失样本中取值为a^v所占比例 利用这三个概率计算每一个属性 a 的信息增益 Gain(D,a) = \rho \times (Ent(D^n) - \sum_{v=1}^{V}{r_v}^~Ent({D_v}^n)) 更新含有缺失值的样本的权值，并划入每一个结点 将划入的数据权值更新为 {r^n}_v * w_x, 相当于将数据依概率进行划分 后续与普通的操作相同 注意： 更新权值相当于以不同的概率将样本划分进每一个结点 因为含有缺失值的样本可能接下来还要参加其他属性的信息增益的计算，而计算中会涉及权值，所以 权值 相当于对带有缺失值的样本的 影响力 4.6 多变量决策树相当于对特征进行了一个线性的组合，之前的特征是单变量，组合之后单个结点相当于一个 线性的分类器 4.7 习题思考1. 以“最小训练误差”作为划分选择的缺点 由于训练集总会与模型之间存在误差，如果选择最小误差，则会很大程度的拟合为符合训练集的决策树，从而导致严重的过拟合 2. 对比使用递归、栈（非递归）、队列（非递归）三种结构生成决策树的优缺点 缺点： 使用递归的方法会保存大量的参数信息，而在参数中有大量的数据信息，从而会导致内存溢出 如果使用栈的结构，不能完成以MaxNode的约束，会生成畸形的决策树（只有一边），只能完成以MaxDepth的约束 如果使用队列的结构，只能完成BFS即以MaxNode的约束 优点： 使用递归的方法代码更加简洁，易读 使用栈的结构有利于后剪枝 使用队列的结构，由于入队之前必须进行划分，如果是用预剪枝则会很方便，因为可以比较划分与否的验证集精度，而栈则做不到 当数据属性相对较多，属性不同取值相对较少时，树会比较宽，此时深度优先所需内存较小，反之宽度优先较小。 第5章 神经网络后续再写 第6章 支持向量机支持向量机内容顺序主要根据《统计学习方法》中的章节来安排，主要内容也由《统计学习方法》展开，其中穿插入西瓜书的知识点 6.1 概念 SVM是一种 二分类 模型 学习算法是 求解凸二次规划的最优化算法 模型分类三类 线性可分支持向量机：当数据集线性可分（硬间隔支持向量机） 线性支持向量机：当数据集近似线性可分（软间隔支持向量机） 非线性支持向量机：使用核技巧 核函数 表示将输入从输入空间映射到特征空间得到的特征向量之间的内积 6.2 线性可分支持向量机 输入空间 输入的数据向量所在的空间 特征空间 特征向量所在的空间 支持向量机的学习是在特征空间中进行的，线性可分SVM将输入空间中的输入线性映射为特征空间中的特征向量，非线性可分SVM将输入非线性映射为特征向量 算法思想 利用间隔最大化求最优分离超平面，解是唯一的 能够正确划分训练数据集并且 几何间隔 最大 学习到的分离超平面： w^* * x + b^* = 0最终的分离决策函数使用符号函数 6.2.1 函数间隔和几何间隔 函数间隔 \hat{\gamma}_i = y_i(w*x_i+b)其中y_i用来表示分类预测的正确性, w*x_i+b 用来表示分类的确信度（换言之就是点到分类超平面的距离） 定义超平面的函数间隔为数据集D中所有样本的函数间隔中的最小值，即： \hat{\gamma} = \min_{i=1,...,N}\hat{\gamma}_i 几何间隔 由于成倍缩放w,b会导致函数间隔成倍缩放，所以 w 加上约束，如约束为：\Vert{w}\Vert = 1， 可以导出几何间隔 函数间隔与几何间隔的关系 \gamma = \frac{\hat{\gamma}}{\Vert{w}\Vert}6.2.2 间隔最大化 直观解释 使得分离有足够的确信度 [ 使得（离分离超平面 最近 的数据点）离超平面 足够远 ] 可以得到求解式： \max{\frac{\hat{\gamma}}{\Vert{w}\Vert}}_{w,b}s.t. y_i(w*x_i+b) \geq \hat{\gamma}, i = i,2,...,N令 \hat{\gamma} = 1,得到 线性可分支持向量机的最优化问题(凸二次优化问题), 具体推导见《方法》P97： \min_{w,b} \frac{1}{2}{\Vert{w}\Vert}^2s.t. y_i(w*x_x+b) - 1 \geq 0, i=1,2,...,N6.2.3 学习的对偶算法求解 线性可分支持向量机的最优化问题 使用 拉格朗日对偶性，令原式为原始问题，通过求解对偶问题得到原始问题的最优解 优点 对偶问题更容易求解 自然引入核函数（在对偶式中出现了内积） 推导过程见 《统计学习方法》 P103 其中几个重要的点： 支持向量是 {\alpha_i}^* > 0 的点 对偶学习问题的KKT条件 KKT的对偶互补条件 {\alpha}^* \geq 0 实际应用中b的求解 理论上使用任意一个数据x_j就能算出 b 在实际应用中，采用更加鲁棒的做法:使用所有 支持向量 求解的平均值,S代表支持向量的集合 b = \frac{1}{S}\sum_{s\in{S}}(\frac{1}{y_s} - \sum_{i\in{S}}\alpha_iy_ix_i^Tx_s)6.3 线性支持向量机与软间隔最大化6.3.1 线性支持向量机 引入 大部分数据并不满足完全线性可分，存在一部分数据非线性可分，因此对每一个数据引入一个松弛变量 \xi \geq 0,使得那些不满足线性可分的数据点加上 \xi 变得线性可分 线性支持向量机的原始问题： \min_{w,b,\xi} \frac{1}{2}{\Vert{w}\Vert}^2+C\sum_{i=1}^N{\xi}_is.t. y_i(w*x_i+b) \geq 1 - {\xi}_i, i = 1,2,...N{\xi}_i \geq 0, i=1,2,...N 原始问题的对偶问题 同线性可分的推导相同，先求\min_{w,b,{\xi}}(即分别对w,b,\xi求导，并令导数等于零，求出相应的表达式，代回原式，得到\min_{w,b,{\xi}}L(w,b,\xi,\alpha,\mu)) 再求出\max_{\alpha}\min_{w,b,{\xi}}L(w,b,\xi,\alpha,\mu) 原始问题满足的KKT条件 6.3.2 支持向量 若{\alpha_i}^* < C, 则\xi_i = 0,支持向量恰好落在间隔边界上 若{\alpha_i}^* = C，0]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于Logistic Regression的理解]]></title>
    <url>%2F2018%2F05%2F20%2FLogistic%20Regression%E7%9A%84%E7%90%86%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[今天学习了周志华教授的西瓜书的第三章 - 线性模型，在知乎上看到了一些关于线性模型不错的理解，在此记录和分享一下 关于Logistic Regression处理非线性平面的问题在看到 Andrew Ng 教授的机器学习中关于 Logistic Regression 的视频的时候，看到有关决策边界的内容，看到这个图的时候想着这只是线性的情况，那 Logistic Regression 是否能处理非线性的情况呢？ （原谅当时没看到视频的后面，其实视频的后面有提到非线性的情况） Logistic Regression 其实是以线性回归算法为基础的，只不过Logistic Regression 模型中在计算最终结果时，为了将连续的结果转换为 [0. 1] 的概率值而引入了 sigmoid 函数映射. 这里重点说一下关于 “线性” 的理解这里的“线性”指的是参数 θ 关于模型是线性关系，即以下关系： z=\theta^Tx=\theta_0x_0+\theta_1x_1+\cdots+\theta_nx_n但是，变量 $x_i$ 关于模型却可以是线性的或是非线性的，下面通过一个例子可以解释： 有一个二分类问题： 房子值不值得买（0 -&gt; 值得买; 1 -&gt; 不值得买）此时假设有两个变量 $x_0$ &amp; $x_1$ 分别代表房子的长度和房子的宽度，而特征有两个： $f1=x_1$ (房子的长度) ; $f_2=2x_2$（两倍的房子的宽度），此时可以用公式： z=\theta_0x_0+\theta_12x_1表示，此时无论是模型关于 参数 还是 模型 关于变量$x_0$、 $x_1$都是 线性的关系。如果房子值不值得买符合线性关系，那么这个时候就已经解决问题了，但是，如果问题并不是一个简单的线性模型，即分类平面不是线性的关系（重点来了，这里的不是线性的关系指代的是模型结果关于$x_0$和$x_1$不是线性关系） 这个时候我们可以利用 $x_0$ 和 $x_1$ 构造出一个非线性的关系出来，例如构造出一个新的变量 $x_1=x_1^2+12$ 这个时候模型关于新的变量就不是线性关系了。 但是注意：此时的模型关于参数 θ 任然是线性的，也即最后训练出来的模型任然是一个线性模型，只不过我们构造出了关于模型的非线性特征使得模型能够更好的拟合到数据中的非线性的特点。 这里再举一个栗子： 假如我们要实际预测的问题的分布是这样的： 此时，分布属于非线性的情况，因为我们有两个变量：$x_1$ 和 $x_2$，如果仅用 $x_1$ 和 $x_2$ 的线性组合我们永远也无法很好的拟合这些数据 但是，如果我们将原来的 $x_1$ 和 $x_2$ 映射为：$x_1^{‘}=x_1^2$ 和 $x_2^{‘}=x_2^2$ 这个时候模型关于新的变量 $x_1^{‘}$ 和 $x_2^{‘}$ 就是线性的关系了，我们可以用： z=\theta_0x_1^{'}+\theta1x_2^{'}表示新的模型，从而很好的拟合原有的数据 从上面的例子我们可以看出，在对Logistic Regression建模的过程中，特征的构建显得很重要。当数据分布对应非线性时，我们就需要找到合适的映射函数 $\phi(x)$ 使得模型能够很好的处理非线性决策边界 总结一下 对于Logistic Regression 来说本质上属于线性回归模型，因为模型关于系数 θ 是线性函数 对于分离平面，无论是线性的还是非线性的，Logistic Regression 其实都可以分类，只不过对于非线性的情况下需要自己去寻找关于原变量的一个非线性映射（如例子中的 $x_1\to x_1^2$） 对于非线性情况下的映射函数的构建至关重要，决定了模型能否很好的拟合非线性决策边界 等看到 SVM 的时候会把Logistic Regression 和 SVM 针对线性情况和非线性情况再做一个对比 原知乎的答案如下： 题主提到了SVM，区别是，SVM如果不用核函数，也得像逻辑回归一样，在映射后的高维空间显示的定义非线性映射函数$\phi$,而引入了核函数之后，可以在低维空间做完点积计算后，映射到高维 原知乎连接：逻辑斯蒂回归能否解决非线性分类问题？ - 辛俊波的回答 - 知乎https://www.zhihu.com/question/29385169/answer/44177582]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>Logistic Regression</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pandas中loc,iloc,ix的使用]]></title>
    <url>%2F2018%2F05%2F06%2FPandas%E4%B8%ADloc-iloc-ix%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[Pandas中loc,iloc,ix的使用使用 iloc 从DataFrame中筛选数据 iloc 是基于“位置”的Dataframe的操作，即主要基于下标的操作 简单使用Pandas中的 iloc 是用基于整数的下标来进行数据定位/选择iloc 的语法是 data.iloc[&lt;row selection&gt;, &lt;column selection&gt;], iloc 在Pandas中是用来通过数字来选择数据中具体的某些行和列。你可以设想每一行都有一个对应的下标（0,1,2，…)，通过 iloc 我们可以利用这些下标去选择相应的行数据。同理，对于行也一样，想象每一列也有对应的下标(0,1,2,…),通过这些下标也可以选择相应的列数据。 在iloc中一共有 2 个 “参数” -行选择器 和 -列选择器，例如： 12345678910# 使用DataFrame 和 iloc 进行单行/列的选择# 行选择：data.iloc[0] # 数据中的第一行data.iloc[1] # 数据中的第二行data.iloc[-1] # 数据中的最后一行# 列选择：data.iloc[:, 0] # 数据中的第一列data.iloc[:, 1] # 数据中的第二列data.iloc[:, -1] # 数据中的最后一列 行列混合选择iloc 同样可以进行和列的混合选择，例如： 12345# 使用 iloc 进行行列混合选择data.iloc[0:5] # 数据中的第 1-5 行data.iloc[:, 0:2] # 选择数据中的前2列和所有行data.iloc[[0, 3, 6, 24], [0, 5, 6]] # 选择第 1,4,7,25行 和 第 1,6,7 列data.iloc[0:5, 5:8] # 选择第1-6行 和 6-9列 使用 iloc 注意以下两点： 如果使用iloc只选择了单独的一行会返回 Series 类型，而如果选择了多行数据则会返回 DataFrame 类型，如果你只选择了一行，但如果想要返回 DataFrame 类型可以传入一个单值list,具体例子看图： 当你使用 [1:5] 这种语法对数据进行切片的时候，要注意只选择了 1,2,3,4 这 4 个下标，而 5 并没有被包括进去，即使用[x:y]选择了下标从 x 到 y-1 的数据 实际工作中，其实很少用到 iloc ，除非你想选择第一行( data.iloc[0] ) 或者 最后一行( data.iloc[-1] ) 使用 loc 从DataFrame中筛选数据可以在以下2中情况下使用 ioc： 使用 基于标签(列头)的下标的 查找 使用 boolean / 有条件的 查找 使用 loc 的语法和 iloc 一样：data.loc[&lt;row selection&gt;, &lt;column selection&gt;] 使用基于标签(列头)的下标数据选择使用 loc 进行数据选择是基于下标的（如果有的话），可以使用 df.set_index() 来设置下标, loc 方法直接通过下标来选择行。例如将”last_name”这一列设置为下标： 1data.set_index("last_name", inplace=True) 效果如图： 现在我们已经将下标设置为”last_name”，这样我们就可以根据”last_name”选择不同的数据了，使用 data.loc[&lt;label&gt;] 同样的可以查找单个值或者多个值，例子如图： 注意，第一个样例代码返回的是 Series 类型，而第二个样例代码返回的是 DataFrame 类型，同样你也可以通过传递一个单值list来返回一个 DataFrame 类型的数据 当然也可以使用 loc 对列进行选择，同时可以选择对列使用 “ : “进行切片选择，效果如图： 同时，你还可以使用 “ : “ 对下标进行切片选择，例如 data.loc[&#39;Bruch&#39;:&#39;Julio&#39;] 会选择从下标为’Bruch’到下标为’Julio’ 的所有行，例如： 123456789# 选择下标值为'Andrade' 和 'Veness',并且从'city'到'email'的所有列data.loc[['Andrade', 'Veness'], ['city':'email']]# 选择和之前相同的行，但只选择'first_name', 'address' 和 'city'这3列data.loc['Andrade':'Veness', ['first_name', 'address', 'city']]# 将下标切换为'id'data.set_index('id', inplace=True) # 在原有数据源上修改# 选择下标（'id'）= 487 的行data.loc[487] 注意：最后一行代码：data.loc[487] 不等价于 data.iloc[487], 前者是选择 ‘id’ = 487 的行，而后者是选择第488行，DataFrame的索引可以是数字顺序的，也可以是字符串或多值的。 使用Boolean / 逻辑判断选择数据使用 boolean 数组进行条件选择是较为常用的手段，使用boolean下标或者逻辑表达式，你可以传递给 loc 一个值为 True/False 的Series或者数组来选择那些 Series或者数组中值为 True 的行。 较多情况下，语句 data[&quot;first_name&quot; == &#39;Antonio&#39;] 会返回一个值为 True/False 的 Series 类型数据，其中 “True” 代表这一行中的 “first_name” 值为 “Antonio”，这些 boolean数组可以直接如图所示传递给 loc 方法： 和之前一样，可以传递给 loc 第2个”参数”用来选择某些列，可以是列举的列名，也可以是用 “ : “ 切片的连续列，如图： 同样要注意：如果只选择了单独的一列，返回的是 Series 类型，同样传递一个单值list可以返回 DataFrame 类型，如图： 通过以下代码可以很好的理解 loc 的使用： 12345678910111213141516171819202122232425# 选择 first_name 为Antonio,并且从 'city' 到 'email'的所有列data.loc[data['first_name'] == 'Antonio', 'city':'email']# 选择那些 email的值中是以 "hotmail.com" 结尾的行，同时选择所有列data.loc[data['email'].str.endswith("hotmail.com")] # 选择那些 "last_name" 等于某些值的行data.loc[data['first_name'].isin(['France', 'Tyisha', 'Eric'])] # 选择 first_name = 'Antonio' 并且 email 是以 "gmail.com"结尾的行data.loc[data['email'].str.endswith("gmail.com") &amp; (data['first_name'] == 'Antonio')] # select rows with id column between 100 and 200, and just return 'postal' and 'web' columns# 选择那些 id 从100到200的行，并且只返回 'postal' 和 'web' 这两列data.loc[(data['id'] &gt; 100) &amp; (data['id'] &lt;= 200), ['postal', 'web']] # lambda函数产生的 True/False 同样可以使用到 loc 中# 选择那些公司名为4个单词的行data.loc[data['company_name'].apply(lambda x: len(x.split(' ')) == 4)] # 为了代码更加清晰， 选择也可以在 .loc 之外进行# 在 .loc 之外单独生成一个变量idx = data['company_name'].apply(lambda x: len(x.split(' ')) == 4)# 只选择 idx 值为True的那些行，并且只选择'email', 'first_name', 'company'这3列data.loc[idx, ['email', 'first_name', 'company']] 顺便说一下Pandas中 map(), apply() 和 applymap()的区别 map() 是 Series 中的函数，DataFrame 中是没有 map() 的，map() 将函数应用于Series中的每一个元素 apply() 和 applymap() 是 DataFrame 中的函数，而在Series中是没有的。他们的区别在于： apply() 将函数作用于DataFrame中的 每一个行或者列，而 applymap() 会将函数作用于DataFrame中的 每一个元素。 使用 loc 修改 DataFrame 中的数据你可以像使用 loc 查询数据那样对数据进行修改，这个操作不会返回新的数据对象而是直接在原数据上进行修改。通过这个操作，你可以根据不同的情况对数据进行修改： 12# 修改 'id' &gt; 2000 的数据中的 'first_name' 为 "John"data.loc[data['id'] &gt; 2000, "first_name"] = "John" 使用 ix 进行选择 现在pandas官方已经不推荐使用 ix 进行选择了，并且将会在 0.20.1版本从Pandas中丢弃]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop 伪分布式环境搭建]]></title>
    <url>%2F2018%2F05%2F02%2FHadoop-%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[Hadoop伪分布式安装步骤JDK 安装（1.8版本及以上） 貌似如果JDK是1.7的话会遇到一些莫名的BUG…比如编译Spark的时候就卡了很久！！ 解压： tar -zxvf jdk-7u79-linux-x64.tar.gz -C ~/app添加到环境变量：~/.bash_profile export JAVA_HOME=/home/hadoop/app/jdk1.7.0_79 export PATH=$JAVA_HOME/bin:$PATH 使得环境变量生效： source ~/.bash_profile验证Java是否配置成功： java -version 安装sshsudo yum install ssh 配置免密码登录 ssh-keygen -t rsa ==&gt; 会生成密码文件到 /home/hadoop/.ssh/id_rsa 中 cp ~/.ssh/id_rsa.pub ~/.ssh/authorized_k eys ==&gt; 将生成的rsa密钥添加到认证密钥中 下载并解压hadoop下载：直接去cdh网站下载 ==&gt; http://archive.cloudera.com/cdh5/cdh/5/ 解压： tar -zxvf hadoop-2.6.0-cdh5.7.0.tar.gz -C /usr/local //将 hadoop 文件夹解压到 /usr/locla 文件夹下 配置文件的修改（hadoop_home/etc/hadoop）hadoop-env.sh 文件中修改： export JAVA_HOME=/usr/local/jdk1.7.0_80 core-site.xml 文件中添加： &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs:// localhost:8020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/hadoop/tmp&lt;/value&gt; &lt;/property&gt; 注：hadoop2.X 版本 将端口从 9000 改为 8020 hdfs-site.xml 文件中添加： &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; 注：只有一个结点，所以副本设置为1 slaves : 有多少个集群，就将主机名添加到slaves 中 启动 HDFS格式化文件系统（仅第一次执行即可，不要重复执行）：./hadoop namenode -format 也可以配置hadoop环境变量之后直接执行： hadoop namenode -format启动hdfs： sbin/start-dfs.sh 验证是否启动成功： jps DataNode SecondaryNameNode NameNode 浏览器访问方式： http://ip:50070 停止 HDFSsbin/stop-dfs.sh 其他问题启动Hadoop HDFS 而DataNode 无法启动的问题可能是之前执行 format 操作之后 导致 DataNode 和 NameNode 的 clusterID不一样&emsp;解决方法：&emsp;1. 进入 Hadoop 中 core-site.xml 文件中配置的Hadoop 文件存放路径，即 hadoop.tmp.dir&emsp;2. 进入 dfs 文件中，将 data 文件夹中 VERSION 文件中的 clusterID 改为 name文件夹中 VERSION文件中的 clusterID 运行 Hadoop 或者 Spark 出现 (null) entry in command string: null chmod 0644当我们尝试将中间计算结果写入本地文件的时候，可能会报出(null) entry in command string: null chmod 0644的错误解决办法一： 下载 winutils.exe 相关文件 从https://github.com/srccodes/hadoop-common-2.2.0-bin将所有的相关文件下载下来 将下载的整个文件夹放到某个目录下， eg：C:\your_folder\ 在程序的开头添加代码： System.setProperty(&quot;hadoop.home.dir&quot;, &quot;c:\\your_folder\\&quot;) //第二个参数就是下载的文件夹所在的路径]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[科学上网教程]]></title>
    <url>%2F2018%2F04%2F23%2F%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[科学上网教程 让我们开始愉快的墙外之旅吧~ 注册 Vultr网站进入 https://www.vultr.com/ 进行注册 具体的注册细节就不细说了 注意！！最近新用户注册会赠送 $25 哟~~先去注册一个PayPal，然后用你的PayPal账号支付$10，就能获得赠送的$25了~ 进入 https://www.vultr.com/ 进行注册 这样就注册成功了 选择左侧栏的 Billing 进入左侧栏的 Servers 选择相应配置 继续配置 这样就配置好了，然后进入详情记录ip地址、root密码等相关属性 使用XShell连接服务器我们这里使用 XShell 连接我们配置好的远程服务器( XShell 使用方法就不赘述了)输入之前的 ip、root 密码等属性，登录远程服务器 锐速更换内核 输入命令： 1rpm -ivh http://soft.91yun.org/ISO/Linux/CentOS/kernel/kernel-firmware-2.6.32-504.3.3.el6.noarch.rpm 然后输入命令： 1rpm -ivh http://soft.91yun.org/ISO/Linux/CentOS/kernel/kernel-2.6.32-504.3.3.el6.x86_64.rpm --force 最后重启服务器：执行命令 1reboot 安装锐速 输入命令： 1wget -N --no-check-certificate https://softs.fun/Bash/ssr.sh &amp;&amp; chmod +x ssr.sh &amp;&amp; bash ssr.sh 选择 14 其他功能 配置锐速 选择安装锐速 安装成功 安装libsodium(chacha20) 选择 4 安装libsodium 输入 y 安装成功 安装ShadowSocksR 选择 1 安装ShadowSocksR 配置端口和密码 端口最好配置为 8开头的！！！密码自己记好就行 配置ShadowSocksR加密方式： 15. chacha20协议插件： 4. auth_aes128_shal混淆插件： 5. tlsl.2_ticket_auth 这里输入 y 之后就连续敲三个回车 漫长等待之后，离成功就差一点点了复制这里的SSR二维码，后面会用到 PC端配置 PC端我们使用 ShadowsocksR 进行墙外之旅这里下载ShadowsocksR文件 链接：git@github.com:danielAck/ShadowsocksR.git放到百度网盘很容易被河蟹… 使用其中的 ShadowsocksR-dotnet4.0.exe 文件 运行后在隐藏图标中找到这个小飞机然后右键点击，选择 粘贴板批量导入ssr://连接… 选择我们配置的服务器，点击确定就可以享受愉快的墙外之旅啦~ 大功告成！]]></content>
      <categories>
        <category>技术</category>
      </categories>
      <tags>
        <tag>科学上网</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于Hashcode以及相关知识]]></title>
    <url>%2F2018%2F04%2F22%2F%E5%85%B3%E4%BA%8EHashcode%E4%BB%A5%E5%8F%8A%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[有关HashCode以及相关知识 今天看了一些有关HashCode的博文，将网上相关的知识整理之后分享一下。 HashCode什么是HashCode HashCode 也即哈希码，是 Java对象 的一个特征码，用它来区分两个Java对象是否相同。HashCode方法的主要作用是为了配合基于散列的集合一起正常运行，这样的散列集合包括HashSet、HashMap以及HashTable。 为什么要用HashCode 因为散列集合中不允许存在重复对象，当向集合中插入对象时，需要判别在集合中是否已经存在该对象，那么这个时候就需要用到HashCode。 便于实现 散列存储。 散列是数组存储方式的一种发展，相比数组，散列的数据访问速度要高于数组，因为可以依据存储数据的部分内容找到数据在数组中的存储位置，进而能够快速实现数据的访问，理想的散列访问速度是非常迅速的，而不像在数组中的遍历过程，采用存储数组中内容的部分元素作为映射函数的输入，映射函数的输出就是存储数据的位置，这样的访问速度就省去了遍历数组的实现，因此时间复杂度可以认为为O(1)，而数组遍历的时间复杂度为O(n)。 ——-[百度百科] 以HashMap为例先提一下 HashMap 中的indexFor()方法：123static int indexFor(int h, int length) &#123; return h &amp; (length-1); &#125; indexFor()方法用于返回插入数据的位置。普通的Hash打散的算法都是mod表的长度，比如h%length,但是 HashMap 却用的是位运算。 因为 HashMap 的初始大小和扩容都是以2的次方来进行的，换句话说length-1换成二进制永远是全部为1，比如 length 为16，则 length-1 为 1111. 而我们知道，任何一个数与它对应的二进制位数上全为1的二进制数进行与运算就是它本身。（例：1001001 ^ 1111111 = 1001001）, 这样保证插入的数据存放在不同的位置，从而最小化哈希冲突，从而实现 散列存储。 再观察 HashMap 中的put()方法：12345678910111213141516171819public V put(K key, V value) &#123; if (key == null) return putForNullKey(value); int hash = hash(key.hashCode()); int i = indexFor(hash, table.length); for (Entry&lt;K,V&gt; e = table[i]; e != null; e = e.next) &#123; Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) &#123; V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; &#125; &#125; modCount++; addEntry(hash, key, value, i); return null; &#125; 其主要思想便是：在键不为空时，根据键对象获取到散列码hash，然后通过散列码得到数组的下标i。在table[i]所表示的 Entry&lt;k,v&gt; 中进行迭代，通过equals()判断该键是否存在，如果存在，则用新的值更新旧的值，返回旧的值；否则将新的键值对添加到 HashMap 中。从这里可以看出，hashCode() 方法的存在是为了减少 equals() 方法的调用次数，从而提高程序效率。 这里我们需要注意到：hashCode() 并不需要总是能够返回唯一的标识码，但是 equals() 方法必须严格地判断两个对象是否相同。 注： 这里贴出的 HashMap 的 put() 方法是网上找的，与我的 jdk1.8 里的内容不一致，但不影响理解。（原因是：JDK1.8中，HashMap采用数组+链表+红黑树实现，当链表长度超过阈值（8）时，将链表转换为红黑树，这样大大减少了查找时间）在 jdk1.8 中 **Entry&amp;lt;k,v&amp;gt;** 是 **Node&amp;lt;K,V&amp;gt;** ，原理其实都一样，**Node&amp;lt;K,V&amp;gt;**是一个静态类，有键值对，还有 **hash** 和 **next** 属性。相当于实现了一个链表。 函数里的 table 就是一个 Node&lt;K,V&gt; 数组。有兴趣的同学可以看一看：123456789101112131415161718192021222324252627282930313233343536373839static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash; final K key; V value; Node&lt;K,V&gt; next; Node(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; this.hash = hash; this.key = key; this.value = value; this.next = next; &#125; public final K getKey() &#123; return key; &#125; public final V getValue() &#123; return value; &#125; public final String toString() &#123; return key + "=" + value; &#125; public final int hashCode() &#123; return Objects.hashCode(key) ^ Objects.hashCode(value); &#125; public final V setValue(V newValue) &#123; V oldValue = value; value = newValue; return oldValue; &#125; public final boolean equals(Object o) &#123; if (o == this) return true; if (o instanceof Map.Entry) &#123; Map.Entry&lt;?,?&gt; e = (Map.Entry&lt;?,?&gt;)o; if (Objects.equals(key, e.getKey()) &amp;&amp; Objects.equals(value, e.getValue())) return true; &#125; return false; &#125;&#125; HashCode的生成 哈希码产生的依据：哈希码并不是完全唯一的，它是一种算法，让同一个类的对象按照自己不同的特征尽量的有不同的哈希码，但不表示不同的对象哈希码完全不同。也有相同的情况，看程序员如何写哈希码的算法。——- [百度百科] 同样以HashMap为例HashMap类中的 hash() 方法:1234static final int hash(Object key) &#123; int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16); &#125; 在 Object 类中hashCode() 是一个native方法，意味着方法的实现和硬件平台有关，默认实现和虚拟机有关，对于有些JVM，hashCode()返回的是对象的地址，大多时候JVM根据一定的规则将与对象相关的信息（比如对象的存储地址，对象的字段等）映射成一个数值，并返回。而在 HashMap中，会调用 Key 所属类实现的 hashCode()方法获得 hashCode。 hash() 是一个返回int值的本地方法。当向一个容器(我们假设是HashMap)中插入一个对象时，怎样判断容器中是否已经存在该对象了呢？由于容器中的元素可能成千上万，使用equals()方法依次进行比较是非常低效的。散列的价值在于速度，它将键保存在某处，以便能够很快找到。存储一组元素最快的数据结构是数组，所以使用它来存储键的信息（注意是键的信息，而非键本身）。我们希望在Map中保存数量不确定的值，但是如果键的数量被数组的容量限制了，该怎么办呢？ 答案就是：数组不保存键本身，而是通过键对象生成一个数字，将其作为数组的下标，这个数字就是散列码（hashcode），由定义在Object中的、且可能由你的类覆盖的hashCode()方法生成。为解决数组容量被固定的问题，不同的键可以产生相同的下标，这种现象被称为哈希冲突（之前也有说过怎么最小化哈希冲突）。于是，在 HashMap 中查询一个值的过程是：先通过 hash() 计算待插入对象的散列码，然后使用散列码查询数组。对于冲突的处理，常常是通过外部链接，即数组并不直接保存值，而是保存相同 hashCode 的对象链表（之前提到的 Node&lt;K,V&gt;链表），然后依次对链表中的值进行线性查询，这部分查询自然会比较慢。但是，如果散列函数足够好的话，数组的每个位置就只有较少的值。因此，散列机制便可以快速地跳到数组的某个位置，只对很少的元素进行比较。这就是HashMap会如此快的原因。 在Java中，不同的对象有不同的HashCode的生成方法： Object类型 Object类的 hashCode() 返回对象的内存地址经过处理后的结构，由于每个对象的内存地址都不一样，所以哈希码也不一样。 String类型 String类的 hashCode() 根据String类包含的字符串的内容，根据一种特殊算法返回哈希码，只要字符串所在的堆空间相同，返回的哈希码也相同。 123456789101112public int hashCode() &#123; int h = hash; if (h == 0 &amp;&amp; value.length &gt; 0) &#123; char val[] = value; for (int i = 0; i &lt; value.length; i++) &#123; h = 31 * h + val[i]; &#125; hash = h; &#125; return h; &#125; 在String类中有 hashCode 的私有字段 12/** Cache the hash code for the string */ private int hash; // Default to 0 在第一次调用hashCode方法时，字符串的哈希值被计算并且赋值给hash字段，之后再调用hashCode方法便可以直接取hash字段返回。 而计String类型计算 hashCode 的方法也比较简单：以31为权重，每一位为字符的ASCII值进行运算，用自然溢出来等效取模。 题外话：为什么要选择31作为权重？ “之所以选择值31是因为它是奇数素数。如果它是偶数，并且乘法溢出，则信息将丢失，因为2的乘法相当于移位。使用素数的优点不太清楚，但它是传统的。31的一个很好的特性是，可以用一个移位和一个减法来代替乘法，以获得更好的性能：31×i =（i &lt; 5）- i。现代VMS自动完成这种优化。” ——-参考Stack Overflow Integer类型Integer类就比较简单了，返回的哈希码就是Integer对象里所包含的那个整数的数值，例如Integer i1=new Integer(100) , i1.hashCode 的值就是100 。由此可见，2个一样大小的Integer对象，返回的哈希码也一样。 HashMap1234static final int hash(Object key) &#123; int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16); &#125; 暂时先写到这吧~~后序如果还有补充内容的话会继续更新。如有错误，还请指正。]]></content>
      <categories>
        <category>Java学习</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>HashCode</tag>
      </tags>
  </entry>
</search>
